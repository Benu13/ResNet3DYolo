{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create model for level boundary estimation:  \n",
    "- estimating level instances for series  \n",
    "- estimating level box - once we find best performing boxes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import timm\n",
    "import timm_3d\n",
    "from typing import List, Sequence, Tuple, Union, Dict\n",
    "from scipy import ndimage\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from MedicalNet.MedicalNet import Struct, MedNet\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from torchvision.tv_tensors import BoundingBoxes as BB\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postprocessing found bboxes:\n",
    "Include guaranteed information about bboxes in image. \n",
    "-> 5 classes each with one bbox or 1 class with 5 bbox\n",
    "-> If there exist level n-1 and n+1 in image there must also exist level n\n",
    "-> The boxes of levels n-1, n, n+1 must be aligned next to another in heigh dimention\n",
    "-> The boxes must overlap in x dimention to some extend \n",
    "-> If there exist series of boxes for levels n, n+1, n+2 and image height is bigger than mean level heigh than the n+3 level must also exist - same situation in reverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bbox3d():\n",
    "    def __init__(self, x, y, z) -> None:\n",
    "        # 3d box in coordinates of sagittal series\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    \n",
    "    def get_box_in_view_type(self, view_type, d3:bool=False):\n",
    "        if view_type in ['sagittal', 'sagittal_t2']:\n",
    "            return self.get_sagittal(d3)\n",
    "        elif view_type == 'coronal':\n",
    "            return self.get_coronal(d3)\n",
    "        elif view_type == 'axial':\n",
    "            return self.get_axial(d3)\n",
    "    \n",
    "    def get_sagittal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.x[0], self.y[0], self.x[1], self.y[1], self.z[0], self.z[1]]\n",
    "        return [self.x[0], self.y[0], self.x[1], self.y[1]]\n",
    "\n",
    "    def get_coronal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.y[0], self.z[1], self.y[1], self.x[0], self.x[1]]\n",
    "        return [self.z[0], self.y[0], self.z[1], self.y[1]]\n",
    "\n",
    "    def get_axial(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.x[0], self.z[1], self.x[1], self.y[0], self.y[1]]\n",
    "        return [self.z[0], self.x[0], self.z[1], self.x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        # data_info: dict consisting of series types and dataframe with their info\n",
    "        # config: dict - dataset configuration\n",
    "        #TODO: Better overlap in instance dimention\n",
    "        #TODO: smart limit - selecting important slices based on series type and where they usually lay\n",
    "        \n",
    "        self.supress_warnings = config['supress_warinings']\n",
    "        self.used_series_types = config['load_series']\n",
    "        self.study_ids = np.unique(np.concatenate([series.study_id.unique() for series in data_info.values() if series is not None]))\n",
    "        self.data_info = data_info\n",
    "        if not np.all([tp in list(self.data_info.keys()) for tp in self.used_series_types]):\n",
    "            raise Exception(\"Series types to use do not match provided data information.\")\n",
    "        \n",
    "        self.series_out_types = config['series_out_types']\n",
    "        self.current_view = self.series_out_types[0]\n",
    "        self.return_series_type = config['return_series_type']\n",
    "\n",
    "        self.d3 = True #config['3d_box'] if '3d_box' in list(config.keys()) else False\n",
    "        self.preload = config['preload']\n",
    "\n",
    "        self.im_size = config['im_size']\n",
    "        if self.im_size:\n",
    "            self.resize = v2.Resize((self.im_size[1], self.im_size[0]))\n",
    "\n",
    "        self.image_type = config['image_type']\n",
    "\n",
    "        if self.preload and not self.supress_warnings:\n",
    "            print(\"Warning! Preloading of images is turned on. The program will attempt to load whole dataset into memory!\")\n",
    "        \n",
    "        self.transforms = config['transforms']\n",
    "        self.transforms_d = config['transforms_d']\n",
    "        self.normalize = config['normalize']\n",
    "        self.vsa = config['vsa']\n",
    "        \n",
    "        self.dataset_type = config['dataset_type']\n",
    "        self.dataset_path = config['dataset_path']\n",
    "\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "        self._condition_list = ['Left Neural Foraminal Narrowing', \n",
    "                                'Left Subarticular Stenosis', \n",
    "                                'Right Neural Foraminal Narrowing', \n",
    "                                'Right Subarticular Stenosis', \n",
    "                                'Spinal Canal Stenosis']\n",
    "        \n",
    "        self._status_map = {'Normal/Mild': [1., 0., 0.],\n",
    "                            'Moderate': [0., 1., 0.],\n",
    "                            'Severe': [0., 0., 1.]}\n",
    "        \n",
    "        self.level_ind = {'L1/L2': 0, 'L2/L3': 1, 'L3/L4': 2, 'L4/L5': 3, 'L5/S1':4}\n",
    "\n",
    "        self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        self.limit = config['limit_series_len']\n",
    "        self.series_length = 15\n",
    "        \n",
    "        self.x_overhead = config['x_overhead'] if 'x_overhead' in list(config.keys()) else [30, 30]\n",
    "        self.z_overhead = config['z_overhead'] if 'z_overhead' in list(config.keys()) else 0\n",
    "        self.overlap_levels = config['overlap_levels']\n",
    "        self.y_overlap = config['y_overlap'] if self.overlap_levels else 0\n",
    "\n",
    "        self.x_overhead_axial = config['x_overhead_axial'] if 'x_overhead_axial' in list(config.keys()) else [2, 2]\n",
    "        self.z_overhead_axial = config['z_overhead_axial'] if 'z_overhead_axial' in list(config.keys()) else 4\n",
    "        self.overlap_levels_axial = config['overlap_levels_axial'] if 'overlap_levels_axial' in list(config.keys()) else False\n",
    "        self.y_overlap_axial = config['y_overlap_axial'] if self.overlap_levels_axial else 0\n",
    "        \n",
    "        if self.limit and not config['series_len'] and not self.supress_warnings:\n",
    "            print(\"Series length is not specified. The limit is set to 15.\")\n",
    "        elif self.limit and config['series_len']:\n",
    "            self.series_length = config['series_len']\n",
    "            \n",
    "        self.series_type_ind = {}\n",
    "        for s in self.used_series_types:\n",
    "            self.series_type_ind[s] = []\n",
    "\n",
    "        self.rand_bord = config['randomize_borders']\n",
    "        if self.rand_bord:\n",
    "            self.x_overhead = self.x_overhead*2\n",
    "            self.z_overhead = self.z_overhead*2\n",
    "            self.y_overlap = self.y_overlap*2\n",
    "\n",
    "            self.x_overhead_axial = self.x_overhead_axial*2\n",
    "            self.z_overhead_axial = self.z_overhead_axial*2\n",
    "            self.y_overlap_axial = self.y_overlap_axial*2\n",
    "\n",
    "        self.sim_bd = False\n",
    "        self.data = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def get_level_boxes(self, series_info:pd.DataFrame, stype='sagittal'):\n",
    "        #default for each scan has 5 visible levels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        if stype in ['sagittal', 'sagittal_t2']:\n",
    "            for _, row in series_info.iterrows():\n",
    "                sl = np.array(row.slice_locations)\n",
    "                \n",
    "                #z_overhead = -self.z_overhead if row.reversed else self.z_overhead\n",
    "                fr_a = sl[row.present_instances.index(min(row.instance_number))]\n",
    "                too_a = sl[row.present_instances.index(max(row.instance_number))]\n",
    "                \n",
    "                fr = min(fr_a, too_a)\n",
    "                too = max(fr_a, too_a)\n",
    " \n",
    "                z_min = np.argmin(abs(sl - (fr-self.z_overhead )))\n",
    "                z_max = np.argmin(abs(sl - (too+self.z_overhead)))\n",
    "\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.x)-self.x_overhead[0]/row.pixel_spacing[0]), min(row.image_width, max(row.x)+self.x_overhead[1]/row.pixel_spacing[0])],\n",
    "                    y=[max(0, row.level_boundaries[0]-self.y_overlap/row.pixel_spacing[1]), min(row.image_height, row.level_boundaries[1]+self.y_overlap/row.pixel_spacing[1])],\n",
    "                    z=[min(z_max, z_min), max(z_max, z_min)]))\n",
    "                \n",
    "        elif stype=='axial':\n",
    "            y_overlap = self.y_overlap + 0.5 \n",
    "            for _, row in series_info.iterrows():\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.y)-self.x_overhead[0]/row.pixel_spacing[1]), min(row.image_width, max(row.y)+self.x_overhead[1]/row.pixel_spacing[1])],\n",
    "                    y=[max(0, row.present_instances.index(min(row.level_slices))-y_overlap), min(len(row.present_instances), row.present_instances.index(max(row.level_slices))+y_overlap)],\n",
    "                    z=[max(0, min(row.x)-self.z_overhead/row.pixel_spacing[0]), min(row.image_height, max(row.x)+self.z_overhead/row.pixel_spacing[0])]\n",
    "                                ))\n",
    "\n",
    "        return bboxes, np.array(labels)\n",
    "\n",
    "    def set_view(self, new_view):\n",
    "        self.current_view = new_view\n",
    "        \n",
    "    def get_condition_labels(self, series_info:pd.DataFrame):\n",
    "        labels = []\n",
    "        cond_presence_masks = []\n",
    "        level_presence_mask = []\n",
    "        for level, _ in self.level_ind.items():\n",
    "            if not series_info[series_info['level']==level].empty:\n",
    "                labels.append(series_info[series_info['level']==level].iloc[0].status)\n",
    "                cond_presence_masks.append(series_info[series_info['level']==level].iloc[0].presence_mask)\n",
    "                level_presence_mask.append(True)\n",
    "            else:\n",
    "                level_presence_mask.append(False)\n",
    "\n",
    "        return np.array(labels), np.array(cond_presence_masks), np.array(level_presence_mask)\n",
    "\n",
    "    def info2dict(self, series_info, stype=None): #remember axials can be combination of different serieses (sagittals can't)\n",
    "        level0 = series_info.iloc[0]\n",
    "        data_dict = {}\n",
    "        boxes, box_labels = self.get_level_boxes(series_info, stype=stype)\n",
    "        labels, label_level_mask, label_cond_mask = self.get_condition_labels(series_info)\n",
    "\n",
    "        data_dict['study_id'] = level0.study_id\n",
    "        data_dict['series_id'] = level0.series_id\n",
    "        data_dict['width'] = level0.image_width\n",
    "        data_dict['height'] = level0.image_height\n",
    "        data_dict['reversed'] = level0.reversed\n",
    "        data_dict['series_type'] = stype\n",
    "        data_dict['pixel_spacing'] = level0.pixel_spacing\n",
    "\n",
    "        data_dict['boxes'] = boxes \n",
    "        data_dict['files'] = [f\"{self.dataset_path}/{data_dict['study_id']}/{data_dict['series_id']}/{instance}.{self.image_type}\" for instance in level0.present_instances]\n",
    "        data_dict['box_labels'] = box_labels\n",
    "        data_dict['labels'] = labels\n",
    "        data_dict['label_presence_mask'] = label_level_mask\n",
    "        #data_dict['level_presence_mask'] = label_cond_mask\n",
    "        \n",
    "        return data_dict\n",
    "   \n",
    "    def prepare_data(self):\n",
    "        # prepare paths for every image to load\n",
    "        with tqdm(total=len(self.study_ids), desc=\"Preparing data: \") as pbar:\n",
    "            for study_id in self.study_ids:\n",
    "                study_dict = dict(\n",
    "                                sagittal=[], \n",
    "                                sagittal_t2=[], \n",
    "                                axial=[])\n",
    "                present = 0\n",
    "                for stype in self.used_series_types:\n",
    "                    for series_id in self.data_info[stype].query(f'study_id == {study_id}').series_id.unique():\n",
    "                        present += 1\n",
    "                        ddict = self.info2dict(self.data_info[stype][self.data_info[stype].series_id==series_id], stype=stype)\n",
    "                        if self.preload:\n",
    "                            ddict = self.preload_series(ddict)\n",
    "                        study_dict[stype].append(ddict)\n",
    "                \n",
    "                if present == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.data.append(study_dict)\n",
    "                pbar.update(1)\n",
    "\n",
    "    def split_to_series(self):\n",
    "        temp = []\n",
    "        batches = []\n",
    "        i=0\n",
    "        for data in self.data:\n",
    "            batch = []\n",
    "            for series in data.values():\n",
    "                if series:\n",
    "                    temp+=series\n",
    "                    batch.append(i)\n",
    "                    i+=1\n",
    "            batches.append(batch)\n",
    "\n",
    "        self.data = temp\n",
    "        self.batches = batches\n",
    "    \n",
    "    def drop_healthy(self, drop=0.5):\n",
    "        to_del = []\n",
    "        for i, data in enumerate(self.data):\n",
    "            labels = torch.tensor(data['labels'])\n",
    "            labels = labels.to(dtype=torch.int64).argmax(-1)\n",
    "            if labels[:,4].sum(0) < 1:\n",
    "                if random.random() > drop:\n",
    "                    to_del.append(i)\n",
    "        self.data = [data for i, data in enumerate(self.data) if i not in to_del]\n",
    "\n",
    "\n",
    "    def change_img_view(self, img, current_view, new_view):\n",
    "        if current_view in ['sagittal', 'sagittal_t2']:\n",
    "            if new_view in ['sagittal', 'sagittal_t2']:\n",
    "                return img\n",
    "            elif new_view=='coronal':\n",
    "                return img.transpose(0, 3, 2, 1) # n, h, w -> w, h, n\n",
    "            elif new_view=='axial':\n",
    "                return img.transpose(0, 2, 3, 1) #n, h, w -> h, n, w\n",
    "        elif current_view=='axial':\n",
    "            if new_view =='sagittal':\n",
    "                return img.transpose(2, 0, 1) # n, h, w -> w, n, h\n",
    "            elif new_view =='coronal':\n",
    "                return img.transpose(2, 1, 0) # n, h, w -> h, n, w\n",
    "            elif new_view=='axial':\n",
    "                return img\n",
    "            \n",
    "            \n",
    "    def preload_series(self, data):\n",
    "\n",
    "        boxes = np.array([box.get_box_in_view_type(data['series_type'], d3 = True) for box in data['boxes']], dtype=int)\n",
    "        level_labels = data['box_labels']\n",
    "        #print(boxes)\n",
    "        oimg = np.zeros((len(data['files']), data['height'], data['width']), dtype = np.float)\n",
    "        for i, path in enumerate(data['files']):  \n",
    "            try:      \n",
    "                oimg[i,:,:] = np.array(Image.open(path), dtype = np.float)\n",
    "            except ValueError:\n",
    "                temp = np.array(Image.open(path), dtype = np.float) \n",
    "                oimg[i,:temp.shape[0],:temp.shape[1]] = temp\n",
    "                del temp\n",
    "\n",
    "        #oimg = self.__itensity_normalize_one_volume__(oimg)\n",
    "\n",
    "        # split to boxes \n",
    "        imgs = [np.zeros((self.series_length, self.im_size[1], self.im_size[0]))]*5\n",
    "        for i, (box, label) in enumerate(zip(boxes, level_labels)):\n",
    "            imgs[label] = oimg[\n",
    "                                int(max(0, box[4]-2)): int(min(oimg.shape[0], box[5]+2)), \n",
    "                                int(max(0, box[1]-10)): int(min(oimg.shape[1], box[3]+10)), \n",
    "                                int(max(0, box[0]-10)): int(min(oimg.shape[2], box[2]+10))\n",
    "                                ]\n",
    "\n",
    "        data['series'] = imgs\n",
    "        return data\n",
    "\n",
    "         \n",
    "    def load_series(self, data) -> np.ndarray:\n",
    "        #print(data)\n",
    "        boxes = np.array([box.get_box_in_view_type(data['series_type'], d3 = True) for box in data['boxes']], dtype=int)\n",
    "        level_labels = data['box_labels']\n",
    "        #print(boxes)\n",
    "        oimg = np.zeros((len(data['files']), data['height'], data['width']), dtype = float)\n",
    "        for i, path in enumerate(data['files']):  \n",
    "            try:      \n",
    "                oimg[i,:,:] = np.array(Image.open(path), dtype = float)\n",
    "            except ValueError:\n",
    "                temp = np.array(Image.open(path), dtype = float) \n",
    "                oimg[i,:temp.shape[0],:temp.shape[1]] = temp\n",
    "                del temp\n",
    "\n",
    "        #oimg = self.__itensity_normalize_one_volume__(oimg)\n",
    "\n",
    "        if self.sim_bd: # resize first to simulate bbox detector output\n",
    "            oimg, boxes = self.__resize_data__(oimg, boxes)\n",
    "\n",
    "        # split to boxes \n",
    "        imgs = np.zeros((5, self.series_length, self.im_size[1], self.im_size[0]), dtype=float)\n",
    "        for i, (box, label) in enumerate(zip(boxes, level_labels)):\n",
    "            if self.rand_bord:\n",
    "                x_overhead = self.x_overhead_axial if data['series_type'] == 'axial' else self.x_overhead\n",
    "                y_overhead = self.y_overlap_axial if data['series_type'] == 'axial' else self.y_overlap\n",
    "                z_overhead = self.z_overhead_axial if data['series_type'] == 'axial' else self.z_overhead\n",
    "\n",
    "                ig = oimg[\n",
    "                                    int(max(0, box[4]+np.random.randint(-2,2))): int(min(oimg.shape[0], box[5]+np.random.randint(-2,2))), \n",
    "\n",
    "                                    int(max(0, box[1]+np.random.randint(-x_overhead[0],x_overhead[0])/data['pixel_spacing'][1])): \n",
    "                                    int(min(oimg.shape[1], box[3]+np.random.randint(-x_overhead[1],x_overhead[1])/data['pixel_spacing'][1])), \n",
    "\n",
    "                                    int(max(0, box[0]+np.random.randint(-y_overhead,y_overhead)/data['pixel_spacing'][0])): \n",
    "                                    int(min(oimg.shape[2], box[2]+np.random.randint(-y_overhead,y_overhead)/data['pixel_spacing'][0]))\n",
    "                                    ]\n",
    "                \n",
    "                try:\n",
    "                    ig = self.__itensity_normalize_one_volume__(self.__resize_data__(ig))\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                ig = self.change_img_view(oimg[box[4]: box[5], box[1]:box[3], box[0]:box[2]], current_view = data['series_type'], new_view='sagittal')\n",
    "                try:\n",
    "                    ig = self.__itensity_normalize_one_volume__(self.__resize_data__(ig))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "            imgs[label] = ig\n",
    "        #print(imgs, imgs.shape)\n",
    "        return imgs\n",
    "\n",
    "    def coarse_dropout_3d(self, volume, max_holes_num, max_hole_size):\n",
    "        # set cut data from volume but leave parts around points with condition of intrest untoutched.\n",
    "        # draw number of holes\n",
    "        hol_num = self.rng.integers(12, volume.shape[0]*max_holes_num)\n",
    "        # for each hole draw its placement on image and size\n",
    "        placement_and_size = self.rng.integers(low=0, \n",
    "                                      high = [[volume.shape[0], volume.shape[1], volume.shape[2], volume.shape[3], \n",
    "                                               max_hole_size[0], max_hole_size[1], max_hole_size[2]]]*hol_num)\n",
    "        for i in range(hol_num):\n",
    "            c = placement_and_size[i,:]\n",
    "            volume[c[0], c[1]:min(volume.shape[1], c[1]+c[4]), c[2]:min(volume.shape[2], c[2]+c[5]), c[3]: min(volume.shape[3], c[3]+c[6])] = 0.\n",
    "        return volume\n",
    "\n",
    "\n",
    "    def __itensity_normalize_one_volume__(self, volume):\n",
    "        \"\"\"\n",
    "        normalize the itensity of an nd volume based on the mean and std of nonzeor region\n",
    "        inputs:\n",
    "            volume: the input nd volume\n",
    "        outputs:\n",
    "            out: the normalized nd volume\n",
    "        \"\"\"\n",
    "        #out = (volume - 0.5*255.)/(0.5 *255.)\n",
    "       \n",
    "        pixels = volume[volume > 0]\n",
    "        if pixels.size==0:\n",
    "            return volume\n",
    "        \n",
    "        mean = pixels.mean()\n",
    "        std  = pixels.std()\n",
    "        out = (volume - mean)/std\n",
    "        #out_random = np.random.normal(0, 1, size = volume.shape)\n",
    "        #out[volume == 0] = out_random[volume == 0]\n",
    "       \n",
    "        return out\n",
    "\n",
    "    def __resize_data__(self, data, bb=None):\n",
    "        \"\"\"\n",
    "        Resize the data to the input size\n",
    "        \"\"\" \n",
    "        \n",
    "        [depth, height, width] = data.shape\n",
    "        scale = [self.series_length*1.0/depth, self.im_size[1]*1.0/height, self.im_size[0]*1.0/width]\n",
    "        #scale_d = [self.series_length*1.0/depth, 1, 1]\n",
    "        if bb:\n",
    "            bb[..., [0,2]] = bb[..., [0,2]]/width*self.im_size[1]\n",
    "            bb[..., [1,3]] = bb[..., [1,3]]/height*self.im_size[0]\n",
    "            bb[..., [4,5]] = bb[..., [4,5]]/depth*self.series_length\n",
    "\n",
    "        data = ndimage.zoom(data, scale, order=1)\n",
    "        #data = ndimage.zoom(data, scale_d, order=0)\n",
    "        #data = self.limit_series(data)\n",
    "        \n",
    "        if bb:\n",
    "            return data, bb\n",
    "        return data\n",
    "    \n",
    "    def __drop_invalid_range__(self, volume, boxes):\n",
    "        \"\"\"\n",
    "        Cut off the invalid area\n",
    "        \"\"\"\n",
    "        zero_value = volume[0, 0, 0]\n",
    "        non_zeros_idx = np.where(volume != zero_value)\n",
    "        \n",
    "        [max_z, max_h, max_w] = np.max(np.array(non_zeros_idx), axis=1)\n",
    "        [min_z, min_h, min_w] = np.min(np.array(non_zeros_idx), axis=1)\n",
    "        \n",
    "        boxes[..., [0,2]] -= min_w\n",
    "        boxes[..., [1,3]] -= min_h\n",
    "        \n",
    "        return volume[min_z:max_z, min_h:max_h, min_w:max_w], boxes \n",
    "    \n",
    "    def limit_series(self, img, z_bb=None):\n",
    "        if self.series_length < img.shape[0]:\n",
    "                if self.d3 and z_bb:\n",
    "                    z_bb = z_bb* self.series_length/img.shape[0]\n",
    "                st = np.round(np.linspace(0, img.shape[0] - 1, self.series_length)).astype(int)\n",
    "                img = img[st,:,:]\n",
    "        elif self.series_length > img.shape[0]:\n",
    "                img = np.pad(img, ((0, self.series_length-img.shape[0]), (0, 0), (0, 0)))\n",
    "        if z_bb:\n",
    "            return img, z_bb\n",
    "        return img\n",
    "\n",
    "    \n",
    "    def prepare_series(self, img, data):\n",
    "    \n",
    "        # img [5, d, h, w]\n",
    "        level_labels = torch.tensor(data['box_labels'], dtype=torch.int64)\n",
    "        cond_labels = torch.zeros((5,5,3), dtype=torch.int64)\n",
    "        cond_masks = torch.zeros((5,5), dtype=torch.int64)\n",
    "        cond_labels[level_labels,...] = torch.tensor(data['labels'], dtype=torch.int64)\n",
    "        cond_masks[level_labels,...] = torch.tensor(data['label_presence_mask'], dtype=torch.int64)\n",
    "\n",
    "        img = torch.tensor(img,dtype=torch.float)\n",
    "        if self.transforms:\n",
    "            # transform in height axis\n",
    "            img = self.transforms(img)\n",
    "            # transform in width axis\n",
    "            if self.transforms_d and torch.rand(1)<0.5:\n",
    "                img = self.transforms_d(img.permute(0, 2, 1, 3))\n",
    "                img = img.permute(0, 2, 1, 3)\n",
    "\n",
    "        if torch.rand(1) < 0.5 and self.vsa:\n",
    "            img = self.coarse_dropout_3d(img,12,[int(self.series_length/3),int(self.im_size[1]/3), int(self.im_size[0]/3)])\n",
    "        \n",
    "        for i, im in enumerate(img):\n",
    "            if torch.count_nonzero(im)==0:\n",
    "                cond_masks[i] *= 0\n",
    "            else: cond_masks[i]=torch.ones_like(cond_masks[i])\n",
    "\n",
    "        return img, level_labels, cond_labels, cond_masks\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDatasetUnited(Dataset):\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        super(BoxDatasetUnited, self).__init__(data_info, config)\n",
    "        self.mix_strategy = config['mix_strategy']\n",
    "        if config['one_label']:\n",
    "            self.box_labels = torch.tensor([0,0,0,0,0], dtype=torch.float)\n",
    "        else:\n",
    "           self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        #split data to individual serieses from dict of study-series_type pairs\n",
    "        self.split_to_series()\n",
    "        #self.drop_healthy()\n",
    "\n",
    "    def drop_series(self, volume,  masks, mask_dropped:bool=True):\n",
    "        # drop one of the serieses and possibly mask conditions primary estimated with it (if mask_dropped)\n",
    "        to_drop = random.choice(list(range(0, len(self.used_series_types))))\n",
    "        volume[to_drop] = torch.zeros((5, self.series_length, self.im_size[1], self.im_size[0]))\n",
    "        if mask_dropped:\n",
    "            if to_drop == 0:\n",
    "                masks[:,[0,2]] = 0\n",
    "            elif to_drop==1:\n",
    "                masks[:,4] = 0\n",
    "            elif to_drop==2:\n",
    "                masks[:,[1,3]] = 0\n",
    "                \n",
    "        return volume, masks\n",
    "\n",
    "    def mirror_sides(self, volume, labels, masks):\n",
    "        # mirror left to right or right to left\n",
    "        if torch.rand(1)>0.5:\n",
    "            #right to left\n",
    "            volume[:, :, :int(volume.size(2)/2), ...] = volume[:, :, int(volume.size(2)/2):, ...].flip(2)\n",
    "            labels[:, [2,3]] = labels[:, [0,1]]\n",
    "            masks[:, [2,3]] = masks[:, [0,1]]\n",
    "        else:\n",
    "            #left to right\n",
    "            volume[:, :, int(volume.size(2)/2):, ...] = volume[:, :, :int(volume.size(2)/2), ...].flip(2)\n",
    "            labels[:, [0,1]] = labels[:, [2,3]]\n",
    "            masks[:, [0,1]] = masks[:, [2,3]]\n",
    "\n",
    "        return volume, labels, masks\n",
    "        \n",
    "    def flip_sides(self, volume, labels, masks):\n",
    "        # flip left/right sides and their labels\n",
    "        volume = volume.flip(2)\n",
    "        labels = labels[:, [2,3,0,1,4]]\n",
    "        masks = masks[:, [2,3,0,1,4]]\n",
    "        return volume, labels, masks\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index: int)->tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        data = self.data[index]\n",
    "        #print(data)\n",
    "        # limit series (if limit==True)\n",
    "        all_type_img = []\n",
    "        all_type_mask = []\n",
    "        all_type_labels = []\n",
    "        type_to_ind = {}\n",
    "        ind = 0\n",
    "        for st in self.used_series_types:\n",
    "            type_to_ind[st] = ind\n",
    "            ind+=1\n",
    "            \n",
    "        all_type_img = torch.zeros((1, 5, self.series_length, self.im_size[1], self.im_size[0]))\n",
    "        \n",
    "        oimg = self.load_series(data)\n",
    "        img, level_presence,  cond_labels, cond_masks = self.prepare_series(oimg, data)\n",
    "        \n",
    "        all_type_img[0, level_presence,...]=img[level_presence,...]\n",
    "        all_type_labels.append(cond_labels)\n",
    "        all_type_mask.append(cond_masks)\n",
    "\n",
    "        try:\n",
    "            all_type_labels = torch.stack(all_type_labels)#.reshape(-1, 5, 4 if not self.d3 else 6)\n",
    "        except:\n",
    "            return all_type_img.permute(1,0,2,3,4), torch.zeros((5,5), dtype=torch.int64), torch.zeros((5,5), dtype=torch.int64)\n",
    "        \n",
    "        all_type_mask = torch.stack(all_type_mask)#.reshape(-1, 5, 1)\n",
    "        all_type_labels,all_type_mask= self.prepare_labels(all_type_labels, all_type_mask)\n",
    "        \n",
    "        #if self.vsa:\n",
    "            #all_type_img = self.transforms(all_type_img.permute(1,0,2,3,4).reshape(5, len(self.used_series_types)*self.series_length, ))\n",
    "            #if torch.rand(1)<0.2:\n",
    "                #all_type_img, all_type_labels, all_type_mask = self.flip_sides(all_type_img, all_type_labels, all_type_mask)\n",
    "            #elif torch.rand(1)<0.2:\n",
    "                #all_type_img, all_type_labels, all_type_mask = self.mirror_sides(all_type_img, all_type_labels, all_type_mask)\n",
    "            #if torch.rand(1) <0.2:\n",
    "                #all_type_img, all_type_mask = self.drop_series(all_type_img, all_type_mask, mask_dropped=False)\n",
    "        \n",
    "        return all_type_img.permute(1,0,2,3,4), all_type_labels.to(dtype=torch.int64), all_type_mask.to(dtype=torch.int64)\n",
    "    \n",
    "    def prepare_labels(self, labels, masks):\n",
    "        labels = labels.sum(axis=0)>0\n",
    "        masks = masks.sum(axis=0)>0\n",
    "        return labels.to(dtype=torch.int64).argmax(-1), masks.to(dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def get_random_by_stype(self):\n",
    "        return self[random.randint(0, len(self)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 128\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.2), v2.RandomHorizontalFlip(p = 0.2)]), #flip\n",
    "    v2.RandomChoice([v2.RandomAffine(degrees=15), v2.RandomAffine(degrees=0, translate=(0.1,0.1), shear=(-3,3,-3,3))]), # translation + shearing\n",
    "    v2.RandomAffine(degrees=0, scale=(0.85,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.GaussianBlur(kernel_size=(3,3), sigma=(0.8, 0.8))]),\n",
    "])\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.GaussianBlur(kernel_size=(3,3), sigma=(1, 1))]),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1,0.1), shear=(-3,3,-3,3)), #resize\n",
    "])\n",
    "\n",
    "train_dataset_config ={\n",
    "    'preload': True, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [128, 128],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset_vl\",\n",
    "    'load_series': ['sagittal_t2'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': None,\n",
    "    'transforms_d': None,\n",
    "    'vsa': False,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['sagittal', 'axial'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal']\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    'randomize_borders':False,\n",
    "    'normalize': True,\n",
    "    'image_type': 'png',\n",
    "    'preload': False,\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 10, #  maximal number of slices in series\n",
    "\n",
    "    'x_overhead': [20,20], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead': 2,\n",
    "    'overlap_levels': False, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 10, # overlap size of levels boundaries (in mm)\n",
    "\n",
    "    'x_overhead_axial': [20,20],\n",
    "    'y_overlap_axial': 40,\n",
    "    'overlap_levels_axial':True,\n",
    "    'z_overhead_axial': 50,\n",
    "    '3d_box': True\n",
    "}\n",
    "\n",
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv').iloc[0:100]\n",
    "\n",
    "data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "train_ids = tsd.study_id.unique()\n",
    "\n",
    "train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "            'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "            'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "\n",
    "bb = BoxDatasetUnited(train_data, train_dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = dict([(0,'r'),(1,'g'), (2,'b'), (3,'m'), (4, 'y')]) # 4646740 41477684\n",
    "\n",
    "\n",
    "inputs, labels, masks = bb[12]\n",
    "i=3\n",
    "inp = inputs[i]\n",
    "print(labels[i])\n",
    "\n",
    "for k in range(10):\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8, 8))\n",
    "    #imgs.append(inputs)\n",
    "\n",
    "    # print ground truth\n",
    "    for i, img in enumerate(inp):\n",
    "        ax.imshow(img[k,:,:].detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SagittalTrainer():\n",
    "    def __init__(self, model, model_params, config, train_data, eval_data) -> None:\n",
    "\n",
    "        self.print_evaluation = config[\"print_evaluation\"] if \"print_evaluation\" in config else False\n",
    "        self.steps_per_plot = config[\"steps_per_plot\"]\n",
    "\n",
    "        if config['train_dataset_config']['mix_strategy'] and len(config['train_dataset_config']['series_out_types'])>1:\n",
    "            print(\"Warning! With mix strategy set to 'combined' and multiple series out types the batch size will be (len(series_out_types)) times bigger.\")\n",
    "\n",
    "        self.checkpoints = config['checkpoints']\n",
    "        self.save_path = config['save_path']\n",
    "        self.step_per_save = config['step_per_save']\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Device set to {self.device}\")\n",
    "\n",
    "        self.model_name = config['model_name']\n",
    "        self.model = model(**model_params).to(self.device)\n",
    "        #self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = config[\"optimizer\"](self.model.parameters(),**config[\"optimizer_params\"])\n",
    "\n",
    "        self.series_len = config['val_dataset_config']['series_len']\n",
    "        self.dataloaders = {'train': torch.utils.data.DataLoader(BoxDatasetUnited(train_data, config['train_dataset_config']), batch_size=config[\"batch_size\"], shuffle=True, num_workers=12, prefetch_factor=1),\n",
    "                           'val': torch.utils.data.DataLoader(BoxDatasetUnited(eval_data, config['val_dataset_config']), batch_size=config[\"batch_size\"], shuffle=False, num_workers=12, prefetch_factor=1)}\n",
    "        \n",
    "        self.max_epochs = config[\"epochs\"]\n",
    "        self.early_stopping = config['early_stopping']\n",
    "        self.early_stopping_tresh = config['early_stopping_treshold']\n",
    "\n",
    "        # scheduler\n",
    "        if config[\"scheduler\"]:\n",
    "            if 'epochs' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['epochs'] = self.max_epochs\n",
    "            if 'steps_per_epoch' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['steps_per_epoch'] = len(self.dataloaders['train'])\n",
    "            self.scheduler = config[\"scheduler\"](self.optimizer,**config[\"scheduler_params\"])\n",
    "            self.one_cycle_sched = self.scheduler.__class__.__name__ == 'OneCycleLR'\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        ## Evaluation metrics\n",
    "        self.series_in_types = config['train_dataset_config']['load_series']\n",
    "        self.out_stypes = config['train_dataset_config']['series_out_types']\n",
    "        self.batch_size = config[\"batch_size\"] * len(self.out_stypes)\n",
    "        self.best_ll = 1\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.save_path, f\"{self.model_name}_best.pt\"))\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.max_epochs}\")\n",
    "            self.train_one_epoch()\n",
    "            self.eval_one_epoch()\n",
    "            #print examples\n",
    "            #checkpoint\n",
    "            if epoch%5==0:\n",
    "                pass\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        with tqdm(self.dataloaders['train'], unit = \"batch\",\n",
    "                    total = len(self.dataloaders['train'])) as tepoch:\n",
    "            for inputs, labels, masks in self.dataloaders['train']:\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    loss, loss_info = self.model.get_loss(inputs.to(self.device).reshape(-1, 5, self.series_len, inputs.shape[-2], inputs.shape[-1]), \n",
    "                                                          labels[:,:,4].to(self.device).reshape(-1, 1), \n",
    "                                                          masks[:,:,4].to(self.device).reshape(-1,1))\n",
    "                    for loss_t, loss_v in loss_info.items():\n",
    "                        metrics[loss_t].append(loss_v.clone().detach().cpu().numpy())\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler and self.one_cycle_sched:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                #update tqdm data\n",
    "                tepoch.set_description(self.metrics_description(metrics, 'train'))\n",
    "                tepoch.update(1)\n",
    "\n",
    "        if self.scheduler and not self.one_cycle_sched:\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "    def eval_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        alabels = []\n",
    "        apreds = []\n",
    "        aweight = []\n",
    "        metrics_d = defaultdict(list)\n",
    "\n",
    "        with tqdm(self.dataloaders['train'], unit = \"batch\",\n",
    "                    total = len(self.dataloaders['val'])) as tepoch:\n",
    "            for inputs, labels, masks in self.dataloaders['val']:\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    preds = self.model.predict(inputs.to(self.device).reshape(-1, 5, self.series_len, inputs.shape[-2], inputs.shape[-1]))\n",
    "                    _, loss_info = self.model.get_loss(inputs.to(self.device).reshape(-1, 1, self.series_len, inputs.shape[-2], inputs.shape[-1]), \n",
    "                                                          labels[:,:,4].to(self.device).reshape(-1, 1), \n",
    "                                                          masks[:,:,4].to(self.device).reshape(-1,1))\n",
    "                    for loss_t, loss_v in loss_info.items():\n",
    "                        metrics_d[loss_t].append(loss_v.clone().detach().cpu().numpy())\n",
    "                \n",
    "                    apreds.append(preds.reshape(-1, 3))\n",
    "\n",
    "                labels=  labels[:,:,4].reshape(-1)\n",
    "                weights = 2**labels\n",
    "                weights[torch.logical_not(masks[:,:,4].reshape(-1))] = 0.\n",
    "                alabels.append(labels)\n",
    "                aweight.append(weights)\n",
    "\n",
    "                #update tqdm data\n",
    "                tepoch.set_description(self.metrics_description(metrics_d, 'train'))\n",
    "                tepoch.update(1)\n",
    "        alabels = torch.cat(alabels, dim=0).cpu().numpy()\n",
    "        apreds = torch.cat(apreds, dim=0).cpu().numpy()\n",
    "        aweight=torch.cat(aweight, dim=0).cpu().numpy()\n",
    "\n",
    "        #prind confusion matrix for every condition\n",
    "        conditions = ['Spinal Canal Stenosis']\n",
    "        \n",
    "        fig, ax = plt.subplots(nrows=1, ncols=len(conditions), figsize=(15,5))\n",
    "        if len(conditions) > 1:\n",
    "            ax = ax.ravel()\n",
    "        else:\n",
    "            ax = [ax]\n",
    "        for i in range(len(conditions)):\n",
    "            cl = alabels[i::len(conditions)]\n",
    "            cpred = apreds[i::len(conditions),:].argmax(-1)\n",
    "            cm = confusion_matrix(cl, cpred)\n",
    "            ax[i].set_title(conditions[i])\n",
    "            ConfusionMatrixDisplay(\n",
    "                confusion_matrix=cm).plot(ax=ax[i], colorbar=False)\n",
    "        plt.show()\n",
    "\n",
    "        ll = log_loss(alabels, apreds, normalize=True, sample_weight=aweight)\n",
    "        if ll < self.best_ll:\n",
    "            self.save_model()\n",
    "            self.best_ll = ll\n",
    "\n",
    "        print(\"Score:\", ll)\n",
    "        \n",
    "        \n",
    "    def metrics_description(self, metrics:dict, phase:str)->str:\n",
    "        outputs = phase + \": ||\"\n",
    "        for k in metrics.keys():\n",
    "            outputs += (\" {}: {:4f} ||\".format(k, np.mean(metrics[k])))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpHead(nn.Module):\n",
    "    \"\"\" MLP classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_classes=1000, mlp_ratio=4, act_layer=nn.ReLU,\n",
    "        norm_layer=nn.LayerNorm, head_dropout=0., bias=True):\n",
    "        super().__init__()\n",
    "        hidden_features = int(mlp_ratio * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n",
    "        self.head_dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.head_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelTimm25d(nn.Module):\n",
    "    def __init__(self, backbone_name, series_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = timm.create_model(\n",
    "                                    backbone_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=True,\n",
    "                                    in_chans=1,\n",
    "                                    num_classes=3,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "        \n",
    "        self.all_channels = self.feature_extractor.feature_info.channels()\n",
    "        self.reduction = self.feature_extractor.feature_info.reduction()\n",
    "        print(self.all_channels, self.reduction)\n",
    "        self.mid_channels = 128\n",
    "        self.out_channels = 16\n",
    "        self.series_dim = series_dim\n",
    "        self.reduce = nn.Sequential(nn.Conv3d(\n",
    "                                        self.all_channels[-1],\n",
    "                                        self.mid_channels,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=(1, 1, 1),\n",
    "                                        ),\n",
    "                                        nn.BatchNorm3d(self.mid_channels),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "\n",
    "                                        nn.Conv3d(\n",
    "                                        self.mid_channels,\n",
    "                                        self.out_channels,\n",
    "                                        kernel_size=(1,1,1),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0, #(1, 1, 1),\n",
    "                                        )\n",
    "                                        )\n",
    "        self.reduce2 = nn.Sequential(nn.Conv3d(\n",
    "                                        self.all_channels[-2],\n",
    "                                        self.mid_channels,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=(1, 1, 1),\n",
    "                                        ),\n",
    "                                        nn.BatchNorm3d(self.mid_channels),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "\n",
    "                                        nn.Conv3d(\n",
    "                                        self.mid_channels,\n",
    "                                        self.out_channels,\n",
    "                                        kernel_size=(1,1,1),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0, #(1, 1, 1),\n",
    "                                        )\n",
    "                                        )\n",
    "        \n",
    "        reduce_1_out = self.out_channels*int(series_dim[0])*int((series_dim[1]/self.reduction[-1])*(series_dim[2]/self.reduction[-1]))\n",
    "        reduce_2_out = self.out_channels*int(series_dim[0])*int((series_dim[1]/self.reduction[-2])*(series_dim[2]/self.reduction[-2]))\n",
    "        self.head = MlpHead(reduce_1_out+reduce_2_out, \n",
    "                              3, 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = x.permute(0,2,1,3,4)\n",
    "        x = self.feature_extractor(x.reshape(b*s*d, 1, h, w))[-2:]\n",
    "        #print(x.shape)\n",
    "        x1 = x[-1].reshape(b, s*d, self.all_channels[-1], self.series_dim[1]//self.reduction[-1], self.series_dim[2]//self.reduction[-1]).permute(0,2,1,3,4)\n",
    "        x2 = x[-2].reshape(b, s*d, self.all_channels[-2], self.series_dim[1]//self.reduction[-2], self.series_dim[2]//self.reduction[-2]).permute(0,2,1,3,4)\n",
    "        x1 = self.reduce(x1).reshape(b, -1)\n",
    "        x2 = self.reduce2(x2).reshape(b, -1)\n",
    "        y = self.head(torch.cat([x1, x2], dim=-1))\n",
    "\n",
    "        return y.reshape(b, 3, -1)\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        \n",
    "        preds = self.forward(input)\n",
    "        labels = labels\n",
    "        w = 3 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none', label_smoothing=0.01) * w\n",
    "        loss = loss.mean()*torch.tensor(preds.shape[0], dtype=loss.dtype).to(preds.device)\n",
    "\n",
    "        return loss.mean(), dict(cross_entropy=loss.mean().clone().detach())\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.forward(input).permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelTimm2d(nn.Module):\n",
    "    def __init__(self, backbone_name, series_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "                                    backbone_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=False,\n",
    "                                    in_chans=series_dim[0]*1,\n",
    "                                    num_classes=3,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = x.permute(0,2,1,3,4)\n",
    "        x = x.reshape(b, s*d, h, w)\n",
    "        y = self.model(x)\n",
    "        return y.reshape(b, 3, -1)\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        \n",
    "        preds = self.forward(input)\n",
    "        w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none', label_smoothing=0.0) * w\n",
    "        loss = loss.mean()*torch.tensor(preds.shape[0], dtype=loss.dtype).to(preds.device)\n",
    "\n",
    "        return loss.mean(), dict(cross_entropy=loss.mean().clone().detach())\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.forward(input).permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelTimm2dLSTM(nn.Module):\n",
    "    def __init__(self, backbone_name, series_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "                                    backbone_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=True,\n",
    "                                    in_chans=series_dim[0],\n",
    "                                    num_classes=3,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "        self.all_channels = self.model.feature_info.channels()\n",
    "        self.reduction = self.model.feature_info.reduction()\n",
    "        print(self.all_channels, self.reduction)\n",
    "        dim = self.all_channels[-1]*int((series_dim[1]/self.reduction[-1])*(series_dim[2]/self.reduction[-1]))\n",
    "        print(dim)\n",
    "        self.lstm = nn.LSTM(dim, 1024, 3, batch_first=True, bidirectional=True)\n",
    "        self.head = MlpHead(2048, 3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = x.reshape(b*s, d, h, w)\n",
    "        y = self.model(x)[-1]\n",
    "        x = y.reshape(b, s, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.head(x).reshape(-1,3)\n",
    "        return x.unsqueeze(-1)\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        \n",
    "        preds = self.forward(input)\n",
    "        w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none', label_smoothing=0.0) * w\n",
    "        loss = loss.mean()*torch.tensor(preds.shape[0], dtype=loss.dtype).to(preds.device)\n",
    "\n",
    "        return loss.mean(), dict(cross_entropy=loss.mean().clone().detach())\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.forward(input).permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5), v2.RandomHorizontalFlip(p = 0.5)]), #flip\n",
    "    v2.RandomAffine(degrees=0, translate=(0.2,0.2)),\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "])\n",
    "train_transforms_depth = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p = 0.5),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.2,0.2)),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((im_size,im_size)), #resize\n",
    "])\n",
    "\n",
    "train_dataset_config ={\n",
    "    'preload': False, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [im_size, im_size],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset\",\n",
    "    'load_series': ['sagittal_t2'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': train_transforms,\n",
    "    'transforms_d': train_transforms_depth,\n",
    "    \n",
    "    'vsa': True,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['sagittal'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal']\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    \n",
    "    'normalize': True,\n",
    "    'image_type': 'png',\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 10, #  maximal number of slices in series\n",
    "    'randomize_borders':True,\n",
    "\n",
    "    'x_overhead': [20,20], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead': 20,\n",
    "    'overlap_levels': True, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 5, # overlap size of levels boundaries (in mm)\n",
    "\n",
    "    'x_overhead_axial': [20,20],\n",
    "    'y_overlap_axial': 1,\n",
    "    'overlap_levels_axial':True,\n",
    "    'z_overhead_axial': 20,\n",
    "    '3d_box': True\n",
    "}\n",
    "\n",
    "val_dataset_config = copy.deepcopy(train_dataset_config)\n",
    "val_dataset_config['transforms'] = None\n",
    "val_dataset_config['load_series'] = ['sagittal_t2']\n",
    "val_dataset_config['transforms_d'] = None\n",
    "val_dataset_config['mix_strategy'] = 'combined'\n",
    "val_dataset_config['vsa'] = False\n",
    "val_dataset_config['randomize_borders'] = True\n",
    "val_dataset_config['return_series_type'] = True\n",
    "'''\n",
    "val_dataset_config['x_overhead'] = [2, 10]\n",
    "val_dataset_config['z_overhead'] = 5\n",
    "val_dataset_config['overlap_levels'] = True\n",
    "val_dataset_config['y_overlap'] = 5\n",
    "\n",
    "val_dataset_config['x_overhead_axial'] = [2, 10]\n",
    "val_dataset_config['z_overhead_axial'] = 5\n",
    "val_dataset_config['overlap_levels_axial'] = True\n",
    "val_dataset_config['y_overlap_axial'] = 5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"print_evaluation\": True,\n",
    "    \"steps_per_plot\": 1,\n",
    "\n",
    "    \"checkpoints\": False,\n",
    "    \"save_path\": \"/workspaces/RSNA_LSDC/models_3d_final/model_weight\",\n",
    "    \"step_per_save\":100,\n",
    "    \"model_name\": \"densenet121LSTM_80_80_10\",\n",
    "    \"train_dataset_config\": train_dataset_config,\n",
    "    \"val_dataset_config\": val_dataset_config,\n",
    "\n",
    "    \"epochs\": 37, \n",
    "    \"batch_size\": 5,\n",
    "\n",
    "    \"optimizer\": torch.optim.AdamW, #torch.optim.AdamW,#torch.optim.Adam,\n",
    "    \"optimizer_params\": {'lr':1e-3, 'weight_decay': 1e-3},#, 'weight_decay': 1e-3, 'momentum': 0.98},#, 'momentum': 0.98, 'weight_decay': 1e-3},#, 'momentum':0.98, 'weight_decay':1e-5},#, 'momentum':0.9},\n",
    "    \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, #torch.optim.lr_scheduler.ExponentialLR, #torch.optim.lr_scheduler.OneCycleLR\n",
    "    \"scheduler_params\": {'T_0': 37, 'T_mult': 1, 'eta_min':3e-8}, #{'T_0': 2, 'T_mult': 2, 'eta_min':3e-5}, #{'max_lr': 0.001, 'epochs': None, 'steps_per_epoch':None}, {'gamma':0.9}\n",
    "\n",
    "    \"early_stopping\": False,\n",
    "    \"early_stopping_treshold\": 0.1,\n",
    "    'vsa':False,\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'backbone_name': 'densenet121', \n",
    "    'series_dim': [train_dataset_config['series_len']]+train_dataset_config['im_size'],\n",
    "    'pretrained': True, \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv').iloc[0:3000]\n",
    "def kfoldCV(k, trainer_config, model_config):\n",
    "    model_summaries = []\n",
    "    data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "    data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "    data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "    unique_studies = np.random.permutation(np.array(tsd.study_id.unique()))\n",
    "    if k == 1:\n",
    "        with open('/workspaces/RSNA_LSDC/models_3d_final/train_unique_studies.npy', 'rb') as f:\n",
    "            train = np.load(f)\n",
    "        with open('/workspaces/RSNA_LSDC/models_3d_final/test_unique_studies.npy', 'rb') as f:\n",
    "            test = np.load(f) \n",
    "        folds = [train, test[0:300]]\n",
    "    else:\n",
    "        folds = np.array_split(unique_studies, k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"Fold: {i}\")\n",
    "        train_ids = np.concatenate(folds[:i]+folds[i+1:], axis=0)\n",
    "        train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "        val_data=  {'sagittal': data_sagittal[data_sagittal.study_id.isin(folds[i])],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(folds[i])],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(folds[i])]}\n",
    "        \n",
    "        trainer = SagittalTrainer(ClassModelTimm2dLSTM, model_config, trainer_config, train_data, val_data)\n",
    "        trainer.train()\n",
    "        model_summaries.append(trainer.get_summary())\n",
    "\n",
    "    return model_summaries\n",
    "# convformer_s18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfoldCV(1, trainer_config, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleSeriesClassificator(nn.Module):\n",
    "    # takes whole level data combined from 3d slices from every series type avaliable in form [series_types = 3, d, h, w]\n",
    "    def __init__(self, backbone_name:str, series_dim:list[int], out_preds, out_channels=32, mid_channels=128):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.series_dim = series_dim\n",
    "        self.backbone_name = backbone_name\n",
    "        self.out_preds= out_preds*3\n",
    "\n",
    "        opts = {\n",
    "        'model': 'resnet',\n",
    "        'input_W': series_dim[2],\n",
    "        'input_H': series_dim[1],\n",
    "        'input_D': series_dim[0],\n",
    "        'device': self.device,\n",
    "        'phase': 'train',\n",
    "        'in_c' : 1,\n",
    "        }\n",
    "\n",
    "        model_pretrained_params = {\n",
    "            'resnet_10': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "            'resnet_10_23dataset': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "            'resnet_18': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "            'resnet_18_23dataset': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "            'resnet_34': {'model_depth': 34, 'resnet_shortcut': 'A'},\n",
    "            'resnet_34_23dataset': {'model_depth': 34, 'resnet_shortcut': 'A'}\n",
    "        }\n",
    "\n",
    "        for model_name, model_dict in model_pretrained_params.items():\n",
    "            model_pretrained_params[model_name] = Struct({**model_dict, **opts})\n",
    "\n",
    "        self.feature_extractor = MedNet('resnet_18_23dataset', model_pretrained_params, 1).to(self.device)\n",
    "        self.feature_extractor.init_FE(self.device)\n",
    "\n",
    "        self.all_channels = [64, 128, 256, 512]#self.feature_extractor.feature_info.channels()\n",
    "        self.reduction = [4, 8, 8, 8] #[4,8,16,32] # self.feature_extractor.feature_info.reduction()\n",
    "        self.mid_channels = mid_channels\n",
    "        self.out_channel = out_channels\n",
    "        self.reduce = nn.Sequential(nn.Conv3d(\n",
    "                                        512,\n",
    "                                        mid_channels,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0,#(1, 1, 1),\n",
    "                                        ),\n",
    "                                        nn.BatchNorm3d(mid_channels),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv3d(\n",
    "                                        mid_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0, #(1, 1, 1),\n",
    "                                        bias=False), \n",
    "                                        nn.BatchNorm3d(out_channels),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        )\n",
    "        \n",
    "        print(int((series_dim[1]/self.reduction[-1])*(series_dim[2]/self.reduction[-1])*(series_dim[0]/self.reduction[-1])))\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(out_channels*4*4*4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, self.out_preds),\n",
    "        )                                 \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.reduce(x[-1])\n",
    "        return x, self.pred(x.reshape(b,-1)).reshape(b,3,-1)\n",
    "\n",
    "class ClassModelMednetSplitSeries(nn.Module):\n",
    "    # separate feature extractor for each series\n",
    "    def __init__(self, backbone_name:str, series_dim:list[int], pretrained:bool=True):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.series_dim = series_dim\n",
    "        self.backbone_name = backbone_name\n",
    "\n",
    "\n",
    "        self.feature_extractor_st1 = SingleSeriesClassificator(backbone_name, series_dim, 5, 32, 128)\n",
    "        self.feature_extractor_st2 = SingleSeriesClassificator(backbone_name, series_dim, 5, 32, 128)\n",
    "        self.feature_extractor_ax2 = SingleSeriesClassificator(backbone_name, series_dim, 5, 32, 128)\n",
    "        self.all_c_dim = 3*32\n",
    "\n",
    "        self.expander = nn.Sequential(nn.Conv3d(\n",
    "                                        self.all_c_dim,\n",
    "                                        512,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=(1, 1, 1),\n",
    "                                        ),\n",
    "                                        nn.BatchNorm3d(512),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv3d(\n",
    "                                        512,\n",
    "                                        512,\n",
    "                                        kernel_size=(1,1,1),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0,#(1, 1, 1),\n",
    "                                        bias=False), \n",
    "                                        nn.BatchNorm3d(512),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        )\n",
    "        self.reducer = nn.Sequential(nn.Conv3d(\n",
    "                                        512,\n",
    "                                        128,\n",
    "                                        kernel_size=(3,3,3),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=(1, 1, 1),\n",
    "                                        ),\n",
    "                                        nn.BatchNorm3d(128),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Conv3d(\n",
    "                                        128,\n",
    "                                        32,\n",
    "                                        kernel_size=(1,1,1),\n",
    "                                        stride=(1, 1, 1),\n",
    "                                        padding=0,#(1, 1, 1),\n",
    "                                        bias=False), \n",
    "                                        nn.BatchNorm3d(32),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        )\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Linear(32*4*4*4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 15),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        #raise\n",
    "        st1, pred_st1 = self.feature_extractor_st1(x[:,0,...].unsqueeze(1))\n",
    "        st2, pred_st2 = self.feature_extractor_st2(x[:,1,...].unsqueeze(1))\n",
    "        ax2, pred_ax2 = self.feature_extractor_ax2(x[:,2, ...].unsqueeze(1))\n",
    "        x = torch.cat([st1, st2, ax2], dim=1)\n",
    "        x = self.reducer(self.expander(x))\n",
    "        return self.pred(x.reshape(b,-1)).reshape(b,3,5), (pred_st1, pred_st2, pred_ax2)\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        a_preds, d_preds = self.forward(input)\n",
    "        #foramina loss\n",
    "        if input[:, 0,...].nonzero().size(0) == 0:\n",
    "            w = labels*0\n",
    "        else:\n",
    "            w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "            w[:,[1,3,4]] *= torch.tensor(0.2, dtype=w.dtype)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss_st1 = F.cross_entropy(d_preds[0], labels, reduction='none', label_smoothing=0.) * w\n",
    "        #subarticular loss\n",
    "        if input[:, 2,...].nonzero().size(0) == 0:\n",
    "            w = labels*0\n",
    "        else:\n",
    "            w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "            w[:,[0,2,4]] *= torch.tensor(0.2, dtype=w.dtype)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss_ax2 = F.cross_entropy(d_preds[2], labels, reduction='none', label_smoothing=0.) * w\n",
    "        #subarticular loss\n",
    "        if input[:, 1,...].nonzero().size(0) == 0:\n",
    "            w = labels*0\n",
    "        else:\n",
    "            w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "            w[:,[0,1,2,3]] *= torch.tensor(0.2, dtype=w.dtype)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss_st2 = F.cross_entropy(d_preds[1], labels, reduction='none', label_smoothing=0.) * w\n",
    "\n",
    "\n",
    "        #calcualte all predictions\n",
    "        w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        aloss = F.cross_entropy(a_preds, labels, reduction='none', label_smoothing=0.) * w\n",
    "\n",
    "        loss = aloss.mean()+loss_st1.mean()+loss_ax2.mean()+loss_st2.mean()\n",
    "        metric_d = dict(sum_loss = loss, all_loss=aloss, st1_loss = loss_st1, st2_loss = loss_st2, ax2_loss=loss_ax2)\n",
    "        return loss, metric_d\n",
    "    \n",
    "    def predict(self, input):\n",
    "        x, _ = self.forward(input)\n",
    "        return x.permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransFormer\n",
    "class StarReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    StarReLU: s * relu(x) ** 2 + b\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_value=1.0, bias_value=0.0,\n",
    "        scale_learnable=True, bias_learnable=True, \n",
    "        mode=None, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.scale = nn.Parameter(scale_value * torch.ones(1),\n",
    "            requires_grad=scale_learnable)\n",
    "        self.bias = nn.Parameter(bias_value * torch.ones(1),\n",
    "            requires_grad=bias_learnable)\n",
    "    def forward(self, x):\n",
    "        return self.scale * self.relu(x)**2 + self.bias\n",
    "    \n",
    "class MlpHead(nn.Module):\n",
    "    \"\"\" MLP classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_classes=1000, mlp_ratio=4, act_layer=nn.ReLU,\n",
    "        norm_layer=nn.LayerNorm, head_dropout=0., bias=True):\n",
    "        super().__init__()\n",
    "        hidden_features = int(mlp_ratio * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n",
    "        self.head_dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.head_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in MetaFormer models, eg Transformer, MLP-Mixer, PoolFormer, MetaFormer baslines and related networks.\n",
    "    Mostly copied from timm.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio=2, out_features=None, act_layer=StarReLU, drop=0., bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        in_features = dim\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = int(mlp_ratio * in_features)\n",
    "        drop_probs = (0., 0.)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla self-attention from Transformer: https://arxiv.org/abs/1706.03762.\n",
    "    Modified from timm.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head_dim=32, num_heads=None, qkv_bias=False,\n",
    "        attn_drop=0., proj_drop=0., proj_bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.num_heads = num_heads if num_heads else dim // head_dim\n",
    "        if self.num_heads == 0:\n",
    "            self.num_heads = 1\n",
    "        \n",
    "        self.attention_dim = self.num_heads * self.head_dim\n",
    "\n",
    "        self.qkv = nn.Linear(dim, self.attention_dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, D, H, W, C = x.shape\n",
    "        N = H * W * D\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, D, H, W, self.attention_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "class MetaFormerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of one MetaFormer block.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop=0.\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.token_mixer = Attention(dim=dim, drop=drop)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = Mlp(dim=dim, drop=drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.token_mixer(self.norm1(x))\n",
    "        x = self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
