{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "import timm\n",
    "import timm_3d\n",
    "from typing import List, Sequence, Tuple, Union, Dict\n",
    "from scipy import ndimage\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from torchvision.tv_tensors import BoundingBoxes as BB\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postprocessing found bboxes:\n",
    "Include guaranteed information about bboxes in image. \n",
    "-> 5 classes each with one bbox or 1 class with 5 bbox\n",
    "-> If there exist level n-1 and n+1 in image there must also exist level n\n",
    "-> The boxes of levels n-1, n, n+1 must be aligned next to another in heigh dimention\n",
    "-> The boxes must overlap in x dimention to some extend \n",
    "-> If there exist series of boxes for levels n, n+1, n+2 and image height is bigger than mean level heigh than the n+3 level must also exist - same situation in reverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bbox3d():\n",
    "    def __init__(self, x, y, z) -> None:\n",
    "        # 3d box in coordinates of sagittal series\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    \n",
    "    def get_box_in_view_type(self, view_type, d3:bool=False):\n",
    "        if view_type in ['sagittal', 'sagittal_t2']:\n",
    "            return self.get_sagittal(d3)\n",
    "        elif view_type == 'coronal':\n",
    "            return self.get_coronal(d3)\n",
    "        elif view_type == 'axial':\n",
    "            return self.get_axial(d3)\n",
    "    \n",
    "    def get_sagittal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.x[0], self.y[0], self.x[1], self.y[1], self.z[0], self.z[1]]\n",
    "        return [self.x[0], self.y[0], self.x[1], self.y[1]]\n",
    "\n",
    "    def get_coronal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.y[0], self.z[1], self.y[1], self.x[0], self.x[1]]\n",
    "        return [self.z[0], self.y[0], self.z[1], self.y[1]]\n",
    "\n",
    "    def get_axial(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.x[0], self.z[1], self.x[1], self.y[0], self.y[1]]\n",
    "        return [self.z[0], self.x[0], self.z[1], self.x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        # data_info: dict consisting of series types and dataframe with their info\n",
    "        # config: dict - dataset configuration\n",
    "        #TODO: Better overlap in instance dimention\n",
    "        #TODO: smart limit - selecting important slices based on series type and where they usually lay\n",
    "        \n",
    "        self.supress_warnings = config['supress_warinings']\n",
    "        self.used_series_types = config['load_series']\n",
    "        self.study_ids = np.unique(np.concatenate([series.study_id.unique() for series in data_info.values() if series is not None]))\n",
    "        self.data_info = data_info\n",
    "        if not np.all([tp in list(self.data_info.keys()) for tp in self.used_series_types]):\n",
    "            raise Exception(\"Series types to use do not match provided data information.\")\n",
    "        \n",
    "        self.series_out_types = config['series_out_types']\n",
    "        self.current_view = self.series_out_types[0]\n",
    "        self.return_series_type = config['return_series_type']\n",
    "\n",
    "        self.d3 = True #config['3d_box'] if '3d_box' in list(config.keys()) else False\n",
    "        self.preload = config['preload']\n",
    "                 \n",
    "        if self.preload and len(self.series_out_types)>1:\n",
    "            self.preload = False\n",
    "            if not self.supress_warnings:\n",
    "                print(\"Preloading with mixed series type outputs is not supported. Preloading was turned off.\")\n",
    "\n",
    "        self.im_size = config['im_size']\n",
    "        if self.im_size:\n",
    "            self.resize = v2.Resize((self.im_size[1], self.im_size[0]))\n",
    "\n",
    "        self.image_type = config['image_type']\n",
    "        self.rev_lr = config['rev'] if 'rev' in list(config.keys()) else False\n",
    "        if self.preload and not self.supress_warnings:\n",
    "            print(\"Warning! Preloading of images is turned on. The program will attempt to load whole dataset into memory!\")\n",
    "        \n",
    "        self.transforms = config['transforms']\n",
    "        self.dtransforms = config['dtransforms']\n",
    "        self.normalize = config['normalize']\n",
    "        self.vsa = config['vsa']\n",
    "        \n",
    "        self.dataset_type = config['dataset_type']\n",
    "        self.dataset_path = config['dataset_path']\n",
    "\n",
    "        self._condition_list = ['Left Neural Foraminal Narrowing', \n",
    "                                'Left Subarticular Stenosis', \n",
    "                                'Right Neural Foraminal Narrowing', \n",
    "                                'Right Subarticular Stenosis', \n",
    "                                'Spinal Canal Stenosis']\n",
    "        \n",
    "        self.get_condition = [self._condition_list.index(cond) for cond in config['get_conditions']]\n",
    "        self.used_conditions = config['get_conditions']\n",
    "        self._status_map = {'Normal/Mild': [1., 0., 0.],\n",
    "                            'Moderate': [0., 1., 0.],\n",
    "                            'Severe': [0., 0., 1.]}\n",
    "        \n",
    "        self.level_ind = {'L1/L2': 0, 'L2/L3': 1, 'L3/L4': 2, 'L4/L5': 3, 'L5/S1':4}\n",
    "\n",
    "        self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        self.limit = config['limit_series_len']\n",
    "        self.series_length = 15\n",
    "        \n",
    "        self.x_overhead = config['x_overhead'] if config['x_overhead'] else [30, 30]\n",
    "        self.z_overhead = 0\n",
    "        self.overlap_levels = config['overlap_levels']\n",
    "        self.y_overlap = config['y_overlap'] if self.overlap_levels else 0\n",
    "\n",
    "        self.cond_x_overhead = config['cond_x_overhead']\n",
    "        self.cond_y_overhead = config['cond_y_overhead']\n",
    "        self.cond_z_overhead = config['cond_z_overhead']\n",
    "\n",
    "        if self.limit and not config['series_len'] and not self.supress_warnings:\n",
    "            print(\"Series length is not specified. The limit is set to 15.\")\n",
    "        elif self.limit and config['series_len']:\n",
    "            self.series_length = config['series_len']\n",
    "            \n",
    "        self.series_type_ind = {}\n",
    "        for s in self.used_series_types:\n",
    "            self.series_type_ind[s] = []\n",
    "\n",
    "        self.data = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def get_level_boxes(self, series_info:pd.DataFrame, stype='sagittal'):\n",
    "        #default for each scan has 5 visible levels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        cond_boxes = []\n",
    "        cond_labels = []\n",
    "        cond_state = []\n",
    "        z_overlap = self.z_overhead\n",
    "        if stype in ['sagittal', 'sagittal_t2']:\n",
    "            for _, row in series_info.iterrows():\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                z_min = min(row.present_instances.index(min(row.instance_number)), row.present_instances.index(max(row.instance_number)))\n",
    "                z_max = max(row.present_instances.index(min(row.instance_number)), row.present_instances.index(max(row.instance_number)))\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.x)-self.x_overhead[0]/row.pixel_spacing[0]), min(row.image_width, max(row.x)+self.x_overhead[1]/row.pixel_spacing[0])],\n",
    "                    y=[max(0, row.level_boundaries[0]-self.y_overlap/row.pixel_spacing[1]), min(row.image_height, row.level_boundaries[1]+self.y_overlap/row.pixel_spacing[1])],\n",
    "                    z=[max(0, z_min-z_overlap), min(len(row.present_instances), z_max+z_overlap)]\n",
    "                                ))\n",
    "                c, cl, cs = self.get_condition_boxes(row, stype=stype)\n",
    "                cond_boxes.append(c)\n",
    "                cond_labels.append(cl)\n",
    "                cond_state.append(cs)\n",
    "\n",
    "        elif stype=='axial':\n",
    "            if z_overlap == 0:\n",
    "                z_overlap = 0.5 \n",
    "            for _, row in series_info.iterrows():\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.y)-self.x_overhead[0]/row.pixel_spacing[1]), min(row.image_width, max(row.y)+self.x_overhead[1]/row.pixel_spacing[1])],\n",
    "                    y=[max(0, row.present_instances.index(min(row.level_slices))-z_overlap), min(len(row.present_instances)-z_overlap, row.present_instances.index(max(row.level_slices))+z_overlap)],\n",
    "                    z=[max(0, min(row.x)-self.y_overlap/row.pixel_spacing[0]), min(row.image_height, max(row.x)+self.y_overlap/row.pixel_spacing[0])]\n",
    "                                ))\n",
    "                c, cl, cs = self.get_condition_boxes(row, stype=stype)\n",
    "                cond_boxes.append(c)\n",
    "                cond_labels.append(cl)\n",
    "                cond_state.append(cs)\n",
    "\n",
    "        return bboxes, np.array(labels), cond_boxes, cond_labels, cond_state\n",
    "    \n",
    "    def get_condition_boxes(self, row:pd.DataFrame, stype='sagittal'):\n",
    "        #default for each scan has 5 visible levels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        cond_state = []\n",
    "        z_overlap = 0\n",
    "        if stype in ['sagittal', 'sagittal_t2']:\n",
    "            for condition, x,y,z in zip(row.condition, row.x, row.y, row.instance_number):\n",
    "                cond_z_overhead = self.cond_z_overhead\n",
    "                if 'Left' in condition:\n",
    "                    if row['reversed']:\n",
    "                        cond_z_overhead = [self.cond_z_overhead[1], self.cond_z_overhead[0]]\n",
    "                    else:\n",
    "                        cond_z_overhead = [self.cond_z_overhead[0], self.cond_z_overhead[1]]\n",
    "                if 'Right' in condition:\n",
    "                    if not row['reversed']:\n",
    "                        cond_z_overhead = [self.cond_z_overhead[1], self.cond_z_overhead[0]]\n",
    "                    else:\n",
    "                        cond_z_overhead = [self.cond_z_overhead[0], self.cond_z_overhead[1]]\n",
    "\n",
    "                labels.append(self._condition_list.index(condition))\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, x-self.cond_x_overhead[0]/row.pixel_spacing[0]), min(row.image_width, x+self.cond_x_overhead[1]/row.pixel_spacing[0])],\n",
    "                    y=[max(0, y-self.cond_y_overhead[0]/row.pixel_spacing[1]), min(row.image_height, y+self.cond_y_overhead[1]/row.pixel_spacing[1])],\n",
    "                    z=[max(0, row.present_instances.index(z)-cond_z_overhead[0]), min(len(row.present_instances), row.present_instances.index(z)+cond_z_overhead[1])]\n",
    "                                ))\n",
    "                cond_state.append(row.status[row.all_conditions.index(condition)])\n",
    "        elif stype=='axial':\n",
    "            z_overlap = 0.5\n",
    "            for condition, x,y,z in zip(row.condition, row.x, row.y, row.instance_number):\n",
    "                labels.append(self._condition_list.index(condition))\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, y-self.cond_x_overhead[0]/row.pixel_spacing[1]), min(row.image_width, y+self.cond_x_overhead[1]/row.pixel_spacing[1])],\n",
    "                    y=[max(0, row.present_instances.index(min(row.level_slices))-z_overlap), min(len(row.present_instances)-z_overlap, row.present_instances.index(max(row.level_slices))+z_overlap)],\n",
    "                    z=[max(0, x-self.cond_y_overhead[0]/row.pixel_spacing[0]), min(row.image_height, x+self.cond_y_overhead[1]/row.pixel_spacing[0])]\n",
    "                                ))\n",
    "                cond_state.append(row.status[row.all_conditions.index(condition)])\n",
    "\n",
    "        return bboxes, np.array(labels), cond_state\n",
    "\n",
    "    def set_view(self, new_view):\n",
    "        self.current_view = new_view\n",
    "        \n",
    "    def get_condition_labels(self, series_info:pd.DataFrame):\n",
    "        labels = []\n",
    "        cond_presence_masks = []\n",
    "        level_presence_mask = []\n",
    "\n",
    "        for level, _ in self.level_ind.items():\n",
    "            if not series_info[series_info['level']==level].empty:\n",
    "                labels.append(series_info[series_info['level']==level].iloc[0].status)\n",
    "                cond_presence_masks.append(series_info[series_info['level']==level].iloc[0].presence_mask)\n",
    "                level_presence_mask.append(True)\n",
    "            else:\n",
    "                level_presence_mask.append(False)\n",
    "\n",
    "        return np.array(labels), np.array(cond_presence_masks), np.array(level_presence_mask)\n",
    "\n",
    "    def info2dict(self, series_info, stype=None): #remember axials can be combination of different serieses (sagittals can't)\n",
    "        level0 = series_info.iloc[0]\n",
    "        data_dict = {}\n",
    "        boxes, box_labels, cond_boxes, cond_labels, cond_state = self.get_level_boxes(series_info, stype=stype)\n",
    "        labels, label_level_mask, label_cond_mask = self.get_condition_labels(series_info)\n",
    "\n",
    "        data_dict['study_id'] = level0.study_id\n",
    "        data_dict['series_id'] = level0.series_id\n",
    "        data_dict['width'] = level0.image_width\n",
    "        data_dict['height'] = level0.image_height\n",
    "        data_dict['reversed'] = level0.reversed\n",
    "        data_dict['series_type'] = stype\n",
    "        data_dict['pixel_spacing'] = level0.pixel_spacing\n",
    "\n",
    "        data_dict['boxes'] = boxes \n",
    "        data_dict['files'] = [f\"{self.dataset_path}/{data_dict['study_id']}/{data_dict['series_id']}/{instance}.{self.image_type}\" for instance in level0.present_instances]\n",
    "        data_dict['box_labels'] = box_labels\n",
    "        data_dict['labels'] = labels\n",
    "        data_dict['label_presence_mask'] = label_level_mask\n",
    "        data_dict['cond_boxes'] = cond_boxes\n",
    "        data_dict['cond_labels'] = cond_labels\n",
    "        data_dict['label_cond_mask'] = label_cond_mask\n",
    "        data_dict['cond_state'] = cond_state\n",
    "        \n",
    "        return data_dict\n",
    "   \n",
    "    def prepare_data(self):\n",
    "        # prepare paths for every image to load\n",
    "        with tqdm(total=len(self.study_ids), desc=\"Preparing data: \") as pbar:\n",
    "            for study_id in self.study_ids:\n",
    "                study_dict = dict(\n",
    "                                sagittal=[], \n",
    "                                sagittal_t2=[], \n",
    "                                axial=[])\n",
    "                present = 0\n",
    "                for stype in self.used_series_types:\n",
    "                    for series_id in self.data_info[stype].query(f'study_id == {study_id}').series_id.unique():\n",
    "                        ddict = self.info2dict(self.data_info[stype][self.data_info[stype].series_id==series_id], stype=stype)\n",
    "                        if self.preload:\n",
    "                            ddict = self.preload_series(ddict)\n",
    "                        study_dict[stype].append(ddict)\n",
    "                        present +=1\n",
    "\n",
    "                if present == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.data.append(study_dict)\n",
    "                pbar.update(1)\n",
    "\n",
    "    def split_to_series(self):\n",
    "        temp = []\n",
    "        batches = []\n",
    "        i=0\n",
    "        for data in self.data:\n",
    "            batch = []\n",
    "            for series in data.values():\n",
    "                if series:\n",
    "                    temp+=series\n",
    "                    batch.append(i)\n",
    "                    i+=1\n",
    "            batches.append(batch)\n",
    "\n",
    "        self.data = temp\n",
    "        self.batches = batches\n",
    "    \n",
    "    def change_img_view(self, img, current_view, new_view):\n",
    "        if current_view in ['sagittal', 'sagittal_t2']:\n",
    "            if new_view == 'sagittal':\n",
    "                return img\n",
    "            elif new_view=='coronal':\n",
    "                return img.transpose(2, 1, 0) # n, h, w -> w, h, n\n",
    "            elif new_view=='axial':\n",
    "                return img.transpose(1, 2, 0 ) #n, h, w -> h, n, w\n",
    "        elif current_view=='axial':\n",
    "            if new_view =='sagittal':\n",
    "                return img.transpose(2, 0, 1) # n, h, w -> w, n, h\n",
    "            elif new_view =='coronal':\n",
    "                return img.transpose(1, 0, 2) # n, h, w -> h, n, w\n",
    "            elif new_view=='axial':\n",
    "                return img\n",
    "            \n",
    "    def load_series(self, data) -> np.ndarray:\n",
    "        boxes = np.array([box.get_box_in_view_type(data['series_type'], d3 = True) for box in data['boxes']], dtype=int)\n",
    "        level_labels = data['box_labels']\n",
    "        oimg = np.zeros((len(data['files']), data['height'], data['width']), dtype = np.uint8)\n",
    "        for i, path in enumerate(data['files']):  \n",
    "            try:      \n",
    "                oimg[i,:,:] = np.array(Image.open(path), dtype = np.uint8)\n",
    "            except ValueError:\n",
    "                temp = np.array(Image.open(path), dtype = np.uint8) \n",
    "                oimg[i,:temp.shape[0],:temp.shape[1]] = temp\n",
    "                del temp\n",
    "\n",
    "        # split to boxes \n",
    "        imgs = np.zeros((5,len(self.used_conditions),self.series_length, self.im_size[1], self.im_size[0]))\n",
    "        labels = np.zeros((5,len(self.used_conditions),3))\n",
    "        masks = np.zeros((5,len(self.used_conditions),1))\n",
    "\n",
    "        for i, (box, label, cond_box, cond_labels, cond_state) in enumerate(zip(boxes, level_labels, data['cond_boxes'], data['cond_labels'], data['cond_state'])):\n",
    "            cb = np.array([cb.get_box_in_view_type(data['series_type'], d3 = True) for cb in cond_box], dtype=int)\n",
    "            for c, cl, cs in zip(cb, cond_labels, cond_state):\n",
    "                if self._condition_list[cl] in self.used_conditions:\n",
    "                    labels[i, self.used_conditions.index(self._condition_list[cl]),:] = cs\n",
    "                    masks[i, self.used_conditions.index(self._condition_list[cl]),0] = data['label_presence_mask'][i,cl]\n",
    "                    imgs[i, self.used_conditions.index(self._condition_list[cl]),:,:,:] =  self.__itensity_normalize_one_volume__(self.__resize_data__(oimg[\n",
    "                                        max(0, c[4]+np.random.randint(-2,1)): min(oimg.shape[0], c[5]+np.random.randint(-1,2)),\n",
    "\n",
    "                                        int(max(0, c[1]+np.random.randint(-10,2)*data['pixel_spacing'][0])): \n",
    "                                        int(min(oimg.shape[1], c[3]+np.random.randint(-2, 10)*data['pixel_spacing'][0])), \n",
    "\n",
    "                                        int(max(0, c[0]+np.random.randint(-10,2)*data['pixel_spacing'][1])): \n",
    "                                        int(min(oimg.shape[2], c[2]+np.random.randint(-2,10)*data['pixel_spacing'][1]))\n",
    "                                        ]))\n",
    "                    \n",
    "                    #imgs[i, self.used_conditions.index(self._condition_list[cl]),:,:,:]=  self.__itensity_normalize_one_volume__(self.__resize_data__(oimg[c[4]: c[5], c[1]:c[3], c[0]:c[2]]))\n",
    "                    \n",
    "        return imgs, labels, masks\n",
    "    \n",
    "\n",
    "    def __itensity_normalize_one_volume__(self, volume):\n",
    "        \"\"\"\n",
    "        normalize the itensity of an nd volume based on the mean and std of nonzeor region\n",
    "        inputs:\n",
    "            volume: the input nd volume\n",
    "        outputs:\n",
    "            out: the normalized nd volume\n",
    "        \"\"\"\n",
    "        #out = (volume - 0.5*255.)/(0.5 *255.)\n",
    "\n",
    "        pixels = volume[volume > 0]\n",
    "        if pixels.size==0:\n",
    "            return volume\n",
    "        \n",
    "        mean = pixels.mean()\n",
    "        std  = pixels.std()\n",
    "        out = (volume - mean)/std\n",
    "        #out_random = np.random.normal(0, 1, size = volume.shape)\n",
    "        #out[volume == 0] = out_random[volume == 0]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __resize_data__(self, data, bb=None):\n",
    "        \"\"\"\n",
    "        Resize the data to the input size\n",
    "        \"\"\" \n",
    "        \n",
    "        [depth, height, width] = data.shape\n",
    "        scale = [self.series_length*1.0/depth, self.im_size[1]*1.0/height, self.im_size[0]*1.0/width]\n",
    "        #scale_d = [self.series_length*1.0/depth, 1, 1]\n",
    "        if bb:\n",
    "            bb[..., [0,2]] = bb[..., [0,2]]/width*self.im_size[1]\n",
    "            bb[..., [1,3]] = bb[..., [1,3]]/height*self.im_size[0]\n",
    "            bb[..., [4,5]] = bb[..., [4,5]]/depth*self.series_length\n",
    "\n",
    "        data = ndimage.zoom(data, scale, order=1)\n",
    "        #data = self.limit_series(data)\n",
    "        \n",
    "        if bb:\n",
    "            return data, bb\n",
    "        return data\n",
    "    \n",
    "    def limit_series(self, img, z_bb=None):\n",
    "        if self.series_length < img.shape[0]:\n",
    "                if self.d3 and z_bb:\n",
    "                    z_bb = z_bb* self.series_length/img.shape[0]\n",
    "                st = np.round(np.linspace(0, img.shape[0] - 1, self.series_length)).astype(int)\n",
    "                img = img[st,:,:]\n",
    "        elif self.series_length > img.shape[0]:\n",
    "                img = np.pad(img, ((0, self.series_length-img.shape[0]), (0, 0), (0, 0)))\n",
    "        if z_bb:\n",
    "            return img, z_bb\n",
    "        return img\n",
    "\n",
    "    \n",
    "    def prepare_series(self, img):\n",
    "\n",
    "        img = torch.tensor(img,dtype=torch.float)\n",
    "        if self.transforms:\n",
    "            # transform in height axis\n",
    "            img = self.transforms(img)\n",
    "            # transform in width axis\n",
    "            if torch.rand(1)<0.5 and self.dtransforms:\n",
    "                img = self.dtransforms(img.permute(0, 1, 3, 2, 4))\n",
    "                img = img.permute(0, 1, 3, 2, 4)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDatasetUnited(Dataset):\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        super(BoxDatasetUnited, self).__init__(data_info, config)\n",
    "        self.mix_strategy = config['mix_strategy']\n",
    "        if config['one_label']:\n",
    "            self.box_labels = torch.tensor([0,0,0,0,0], dtype=torch.float)\n",
    "        else:\n",
    "           self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        #split data to individual serieses from dict of study-series_type pairs\n",
    "    \n",
    "    def drop_series(self, volume,  masks, mask_dropped:bool=True):\n",
    "        # drop one of the serieses and possibly mask conditions primary estimated with it (if mask_dropped)\n",
    "        to_drop = random.choice(list(range(0, len(self.used_series_types))))\n",
    "        volume[to_drop] = torch.zeros((5, self.series_length, self.im_size[1], self.im_size[0]))\n",
    "        if mask_dropped:\n",
    "            if to_drop == 0:\n",
    "                masks[:,[0,2]] = 0\n",
    "            elif to_drop==1:\n",
    "                masks[:,4] = 0\n",
    "            elif to_drop==2:\n",
    "                masks[:,[1,3]] = 0\n",
    "                \n",
    "        return volume, masks\n",
    "\n",
    "    def mirror_sides(self, volume, labels, masks):\n",
    "        # mirror left to right or right to left\n",
    "        if torch.rand(1)>0.5:\n",
    "            #right to left\n",
    "            volume[:, :, :int(volume.size(2)/2), ...] = volume[:, :, int(volume.size(2)/2):, ...].flip(2)\n",
    "            labels[:, [2,3]] = labels[:, [0,1]]\n",
    "            masks[:, [2,3]] = masks[:, [0,1]]\n",
    "        else:\n",
    "            #left to right\n",
    "            volume[:, :, int(volume.size(2)/2):, ...] = volume[:, :, :int(volume.size(2)/2), ...].flip(2)\n",
    "            labels[:, [0,1]] = labels[:, [2,3]]\n",
    "            masks[:, [0,1]] = masks[:, [2,3]]\n",
    "\n",
    "        return volume, labels, masks\n",
    "        \n",
    "    def flip_sides(self, volume, labels, masks):\n",
    "        # flip left/right sides and their labels\n",
    "        volume = volume.flip(2)\n",
    "        labels = labels[:, [2,3,0,1,4]]\n",
    "        masks = masks[:, [2,3,0,1,4]]\n",
    "        return volume, labels, masks\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index: int)->tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        adata = self.data[index]\n",
    "        \n",
    "        # limit series (if limit==True)\n",
    "        all_type_img = []\n",
    "        all_type_mask = []\n",
    "        all_type_labels = []\n",
    "        type_to_ind = {}\n",
    "        ind = 0\n",
    "        for st in self.used_series_types:\n",
    "            type_to_ind[st] = ind\n",
    "            ind+=1\n",
    "            \n",
    "        all_type_img = torch.zeros((len(self.used_series_types), 5, len(self.used_conditions), self.series_length, self.im_size[1], self.im_size[0]), dtype=torch.float)\n",
    "        for key, data_list in adata.items():\n",
    "            if data_list:\n",
    "                for data in data_list: #schuffle if you want random in some cases\n",
    "                    if not self.preload:\n",
    "                        try:\n",
    "                            oimg, labels, masks = self.load_series(data)\n",
    "                            oimg = self.prepare_series(oimg)\n",
    "                        except ZeroDivisionError or UnboundLocalError: # one series was skipped in data preparation and now it raises exception ;_;\n",
    "                            print('Series ', data['series_id'], ' dropped.')\n",
    "                            return all_type_img.permute(1,2,0,3,4,5), torch.zeros((5,2)).to(dtype=torch.int64), torch.zeros((5,2,1)).to(dtype=torch.int64)\n",
    "                    all_type_img[type_to_ind[key]] = oimg\n",
    "        # try:\n",
    "        #     oimg = self.prepare_series(oimg)\n",
    "        # except UnboundLocalError:\n",
    "        #     return all_type_img, torch.zeros((5,2)).to(dtype=torch.int64), torch.zeros((5,2,1)).to(dtype=torch.int64)\n",
    "        \n",
    "        labels, masks = self.prepare_labels(labels, masks)\n",
    "        \n",
    "        return all_type_img.permute(1,2,0,3,4,5), labels, masks\n",
    "    \n",
    "    def prepare_labels(self, labels, masks):\n",
    "        labels = torch.tensor(labels)\n",
    "        masks = torch.tensor(masks)\n",
    "        return labels.to(dtype=torch.int64).argmax(-1), masks.to(dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def get_random_by_stype(self):\n",
    "        return self[random.randint(0, len(self)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5), v2.RandomHorizontalFlip(p = 0.5), v2.RandomAffine(degrees=5), \n",
    "                     v2.RandomRotation(degrees=(90,90)), v2.RandomRotation(degrees=(-90,-90))]),\n",
    "    v2.RandomChoice([v2.RandomPerspective(distortion_scale=0.1, p=1.0), v2.RandomResizedCrop(size=(im_size,im_size)), v2.RandomAffine(degrees=0, translate=(0.1,0.1), shear=(-2,2,-2,2))]), # translation + shearing\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.ElasticTransform(alpha=40.0), v2.GaussianBlur(kernel_size=(3,7), sigma=(0.1, 0.7))]),\n",
    "])\n",
    "dtransforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5)]),\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.GaussianBlur(kernel_size=(3,7), sigma=(0.1, 0.7))]),\n",
    "])\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((im_size,im_size)), #resize\n",
    "])\n",
    "\n",
    "train_dataset_config ={\n",
    "    'preload': True, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [im_size, im_size],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset\",\n",
    "    'load_series': ['axial', 'sagittal'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': train_transforms,\n",
    "    'dtransforms': dtransforms,\n",
    "    'vsa': False,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['axial'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal']\n",
    "    'get_conditions':['Left Neural Foraminal Narrowing', 'Right Neural Foraminal Narrowing'], # condition to output from series\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    \n",
    "    'normalize': True,\n",
    "    'image_type': 'png',\n",
    "    'preload': False,\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 6, #  maximal number of slices in series\n",
    "\n",
    "    'x_overhead': [20,20], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead':10,\n",
    "    'overlap_levels': True, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 15, # overlap size of levels boundaries (in mm)\n",
    "    'cond_x_overhead': [18,18],\n",
    "    'cond_y_overhead': [8,8],\n",
    "    'cond_z_overhead': [6,16],\n",
    "\n",
    "    '3d_box': True\n",
    "}\n",
    "\n",
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv').iloc[0:300]\n",
    "\n",
    "data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "train_ids = tsd.study_id.unique()\n",
    "\n",
    "train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "            'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "            'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "\n",
    "bb = BoxDatasetUnited(train_data, train_dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = dict([(0,'r'),(1,'g'), (2,'b'), (3,'m'), (4, 'y')]) # 4646740 41477684\n",
    "\n",
    "\n",
    "inputs, labels, masks = bb[3]\n",
    "\n",
    "\n",
    "inp = inputs[3, 0]\n",
    "print(labels[3, 0])\n",
    "#imgs.append(inputs)\n",
    "\n",
    "# print ground truth\n",
    "for i in range(inp.shape[1]):\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (10, 10))\n",
    "    ax[0].imshow(inp[0, i,:,:].detach().cpu().numpy())\n",
    "    ax[1].imshow(inp[1,i,:,:].detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SagittalTrainer():\n",
    "    def __init__(self, model, model_params, config, train_data, eval_data) -> None:\n",
    "\n",
    "        self.print_evaluation = config[\"print_evaluation\"] if \"print_evaluation\" in config else False\n",
    "        self.steps_per_plot = config[\"steps_per_plot\"]\n",
    "\n",
    "        if config['train_dataset_config']['mix_strategy'] and len(config['train_dataset_config']['series_out_types'])>1:\n",
    "            print(\"Warning! With mix strategy set to 'combined' and multiple series out types the batch size will be (len(series_out_types)) times bigger.\")\n",
    "\n",
    "        self.checkpoints = config['checkpoints']\n",
    "        self.save_path = config['save_path']\n",
    "        self.step_per_save = config['step_per_save']\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Device set to {self.device}\")\n",
    "\n",
    "        self.model_name = config['model_name']\n",
    "        self.model = model(**model_params).to(self.device)\n",
    "        #self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = config[\"optimizer\"](self.model.parameters(),**config[\"optimizer_params\"])\n",
    "\n",
    "        self.series_len = config['val_dataset_config']['series_len']\n",
    "        self.dataloaders = {'train': torch.utils.data.DataLoader(BoxDatasetUnited(train_data, config['train_dataset_config']), batch_size=config[\"batch_size\"], shuffle=True, num_workers=12, prefetch_factor=1),\n",
    "                           'val': torch.utils.data.DataLoader(BoxDatasetUnited(eval_data, config['val_dataset_config']), batch_size=config[\"batch_size\"], shuffle=False, num_workers=12, prefetch_factor=1)}\n",
    "        \n",
    "        self.max_epochs = config[\"epochs\"]\n",
    "        self.early_stopping = config['early_stopping']\n",
    "        self.early_stopping_tresh = config['early_stopping_treshold']\n",
    "\n",
    "        # scheduler\n",
    "        if config[\"scheduler\"]:\n",
    "            if 'epochs' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['epochs'] = self.max_epochs\n",
    "            if 'steps_per_epoch' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['steps_per_epoch'] = len(self.dataloaders['train'])\n",
    "            self.scheduler = config[\"scheduler\"](self.optimizer,**config[\"scheduler_params\"])\n",
    "            self.one_cycle_sched = self.scheduler.__class__.__name__ == 'OneCycleLR'\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        ## Evaluation metrics\n",
    "        self.series_in_types = config['train_dataset_config']['load_series']\n",
    "        self.out_stypes = config['train_dataset_config']['series_out_types']\n",
    "        self.batch_size = config[\"batch_size\"] * len(self.out_stypes)\n",
    "\n",
    "        self.mAP = MeanAveragePrecision(box_format = 'xyxy', iou_type='bbox', extended_summary=True).to(self.device)\n",
    "        self.all_maps = []\n",
    "        self.mAP_split = []\n",
    "        self.best_ll = 1\n",
    "        self.metrics_to_print = ['map', 'map_50', 'map_75', 'map_per_class']\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.save_path, f\"{self.model_name}_best.pt\"))\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.max_epochs}\")\n",
    "            self.train_one_epoch()\n",
    "            self.eval_one_epoch()\n",
    "            #print examples\n",
    "            #checkpoint\n",
    "            if epoch%5==0:\n",
    "                pass\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        with tqdm(self.dataloaders['train'], unit = \"batch\",\n",
    "                    total = len(self.dataloaders['train'])) as tepoch:\n",
    "            for inputs, labels, masks in self.dataloaders['train']:\n",
    "                inputs = inputs.to(self.device).reshape(-1, 1, self.series_len, inputs.shape[-2], inputs.shape[-1])\n",
    "                labels = labels.to(self.device).reshape(-1, 1)\n",
    "                masks = masks.to(self.device).reshape(-1, 1)\n",
    "                valid = [i for i in range(inputs.shape[0]) if (inputs[i].argmax() > 0)]\n",
    "                if len(valid) <1:\n",
    "                    tepoch.update(1)\n",
    "                    continue\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    loss, loss_info = self.model.get_loss(inputs[valid], labels[valid], masks[valid])\n",
    "                    for loss_t, loss_v in loss_info.items():\n",
    "                        metrics[loss_t].append(loss_v.clone().detach().cpu().numpy())\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler and self.one_cycle_sched:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                #update tqdm data\n",
    "                tepoch.set_description(self.metrics_description(metrics, 'train'))\n",
    "                tepoch.update(1)\n",
    "\n",
    "        if self.scheduler and not self.one_cycle_sched:\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "    def eval_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        alabels = []\n",
    "        apreds = []\n",
    "        aweight = []\n",
    "        metrics_d = defaultdict(list)\n",
    "\n",
    "        with tqdm(self.dataloaders['train'], unit = \"batch\",\n",
    "                    total = len(self.dataloaders['val'])) as tepoch:\n",
    "            for inputs, labels, masks in self.dataloaders['val']:\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    inputs = inputs.to(self.device).reshape(-1, 1, self.series_len, inputs.shape[-2], inputs.shape[-1])\n",
    "                    labels = labels.to(self.device).reshape(-1, 1)\n",
    "                    masks = masks.to(self.device).reshape(-1, 1)\n",
    "                    valid = [i for i in range(inputs.shape[0]) if (inputs[i].argmax() > 0)]\n",
    "                    if len(valid) <1:\n",
    "                        tepoch.update(1)\n",
    "                        continue\n",
    "                    inputs = inputs[valid]\n",
    "                    labels = labels[valid]\n",
    "                    masks = masks[valid]\n",
    "                    \n",
    "                    preds = self.model.predict(inputs.to(self.device).reshape(-1, len(self.series_in_types), self.series_len, inputs.shape[-2], inputs.shape[-1]))\n",
    "                    _, loss_info = self.model.get_loss(inputs.to(self.device).reshape(-1, len(self.series_in_types), self.series_len, inputs.shape[-2], inputs.shape[-1]), \n",
    "                                                          labels.to(self.device).reshape(-1, 1), \n",
    "                                                          masks.to(self.device).reshape(-1,1))\n",
    "                    for loss_t, loss_v in loss_info.items():\n",
    "                        metrics_d[loss_t].append(loss_v.clone().detach().cpu().numpy())\n",
    "                \n",
    "                    apreds.append(preds.reshape(-1, 3))\n",
    "\n",
    "                labels=  labels.reshape(-1)\n",
    "                weights = 2**labels\n",
    "                weights[torch.logical_not(masks.reshape(-1))] = 0.\n",
    "                alabels.append(labels)\n",
    "                aweight.append(weights)\n",
    "\n",
    "                #update tqdm data\n",
    "                tepoch.set_description(self.metrics_description(metrics_d, 'train'))\n",
    "                tepoch.update(1)\n",
    "        alabels = torch.cat(alabels, dim=0).cpu().numpy()\n",
    "        apreds = torch.cat(apreds, dim=0).cpu().numpy()\n",
    "        aweight=torch.cat(aweight, dim=0).cpu().numpy()\n",
    "\n",
    "        #prind confusion matrix for every condition\n",
    "        conditions = ['Left Neural Foraminal Narrowing','Left Neural Foraminal Narrowing']\n",
    "        \n",
    "        fig, ax = plt.subplots(nrows=1, ncols=len(conditions), figsize=(15,5))\n",
    "        if len(conditions) > 1:\n",
    "            ax = ax.ravel()\n",
    "        else:\n",
    "            ax = [ax]\n",
    "        for i in range(len(conditions)):\n",
    "            cl = alabels[i::len(conditions)]\n",
    "            cpred = apreds[i::len(conditions),:].argmax(-1)\n",
    "            cm = confusion_matrix(cl, cpred)\n",
    "            ax[i].set_title(conditions[i])\n",
    "            ConfusionMatrixDisplay(\n",
    "                confusion_matrix=cm).plot(ax=ax[i], colorbar=False)\n",
    "        plt.show()\n",
    "\n",
    "        ll = log_loss(alabels, apreds, normalize=True, sample_weight=aweight)\n",
    "        print(\"Score:\", ll)\n",
    "        if ll < self.best_ll:\n",
    "            self.save_model()\n",
    "            self.best_ll = ll\n",
    "        \n",
    "    def metrics_description(self, metrics:dict, phase:str)->str:\n",
    "        outputs = phase + \": ||\"\n",
    "        for k in metrics.keys():\n",
    "            outputs += (\" {}: {:4f} ||\".format(k, np.mean(metrics[k])))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpHead(nn.Module):\n",
    "    \"\"\" MLP classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_classes=1000, mlp_ratio=4, act_layer=nn.ReLU,\n",
    "        norm_layer=nn.LayerNorm, head_dropout=0., bias=True):\n",
    "        super().__init__()\n",
    "        hidden_features = int(mlp_ratio * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n",
    "        self.head_dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.head_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelTimm2d(nn.Module):\n",
    "    def __init__(self, backbone_name, series_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "                                    backbone_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=False,\n",
    "                                    in_chans=series_dim[0]*1,\n",
    "                                    num_classes=3,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = x.permute(0,2,1,3,4)\n",
    "        x = x.reshape(b, s*d, h, w)\n",
    "        y = self.model(x)\n",
    "        return y.reshape(b, 3, -1)\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        preds = self.forward(input)\n",
    "        labels = labels\n",
    "        w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        \n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none', label_smoothing=0.01, weight=torch.tensor([1, 2., 4.], dtype = preds.dtype).to(preds.device)) * masks\n",
    "        loss = loss.mean()*torch.tensor(preds.shape[0], dtype=loss.dtype).to(preds.device)\n",
    "\n",
    "        return loss.mean(), dict(cross_entropy=loss.mean().clone().detach())\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.forward(input).permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelTimm2dLSTM(nn.Module):\n",
    "    def __init__(self, backbone_name, series_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "                                    backbone_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=True,\n",
    "                                    in_chans=series_dim[0],\n",
    "                                    num_classes=3,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "        self.all_channels = self.model.feature_info.channels()\n",
    "        self.reduction = self.model.feature_info.reduction()\n",
    "        print(self.all_channels, self.reduction)\n",
    "        dim = self.all_channels[-1]*int((series_dim[1]/self.reduction[-1])*(series_dim[2]/self.reduction[-1]))\n",
    "        self.lstm = nn.LSTM(dim, 512, 2, batch_first=True, bidirectional=True)\n",
    "        self.head = MlpHead(512*20, 10*3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d, h, w = x.shape\n",
    "        x = x.reshape(b*s, d, h, w)\n",
    "        y = self.model(x)[-1]\n",
    "        x = y.reshape(b, s, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.head(x.reshape(b,-1)).reshape(b,10,3)\n",
    "        return x\n",
    "\n",
    "    def get_loss(self, input, labels, masks):\n",
    "        \n",
    "        preds = self.forward(input)\n",
    "        preds = preds.reshape(-1,3).unsqueeze(-1)\n",
    "        labels = labels.reshape(-1,1)\n",
    "        masks = masks.reshape(-1,1)\n",
    "        w = 2 ** labels # sample_weight w = (1, 2, 4) for y = 0, 1, 2 (batch_size, n)\n",
    "        w[torch.logical_not(masks)] = 0. # set weight for unnanoted conds to 0\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none', label_smoothing=0.0) * w\n",
    "        loss = loss.mean()*torch.tensor(preds.shape[0], dtype=loss.dtype).to(preds.device)\n",
    "\n",
    "        return loss.mean(), dict(cross_entropy=loss.mean().clone().detach())\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.forward(input).permute(0,2,1).softmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 80\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5), v2.RandomHorizontalFlip(p = 0.5), v2.RandomAffine(degrees=5), \n",
    "                     v2.RandomRotation(degrees=(90,90)), v2.RandomRotation(degrees=(-90,-90))]),\n",
    "    v2.RandomChoice([v2.RandomPerspective(distortion_scale=0.2, p=.5), v2.RandomAffine(degrees=0, translate=(0.1,0.1), shear=(-5,5,-5,5))]), # translation + shearing\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.GaussianBlur(kernel_size=(3,7), sigma=(0.1, 0.7))]),\n",
    "])\n",
    "\n",
    "dtransforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5)]),\n",
    "    #v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.GaussianBlur(kernel_size=(3,7), sigma=(0.1, 0.7))]),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((im_size,im_size)), #resize\n",
    "])\n",
    "\n",
    "train_dataset_config ={\n",
    "    'preload': True, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [im_size, im_size],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset\",\n",
    "    'load_series': ['axial'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': train_transforms,\n",
    "    'dtransforms': None,\n",
    "    'vsa': False,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['axial'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal']\n",
    "    'get_conditions':[ 'Left Subarticular Stenosis',  'Right Subarticular Stenosis'], #['Left Neural Foraminal Narrowing', 'Right Neural Foraminal Narrowing'], # condition to output from series\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    \n",
    "    'normalize': True,\n",
    "    'image_type': 'png',\n",
    "    'preload': False,\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 3, #  maximal number of slices in series\n",
    "\n",
    "    'x_overhead': [20,20], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead':10,\n",
    "    'overlap_levels': True, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 15, # overlap size of levels boundaries (in mm)\n",
    "    'cond_x_overhead': [6,9],\n",
    "    'cond_y_overhead': [12,14],\n",
    "    'cond_z_overhead': [4,4],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "val_dataset_config = copy.deepcopy(train_dataset_config)\n",
    "val_dataset_config['transforms'] = None\n",
    "val_dataset_config['dtransforms'] = None\n",
    "val_dataset_config['mix_strategy'] = 'combined'\n",
    "val_dataset_config['vsa'] = False\n",
    "val_dataset_config['randomize_borders'] = True\n",
    "val_dataset_config['return_series_type'] = True\n",
    "'''\n",
    "val_dataset_config['x_overhead'] = [2, 10]\n",
    "val_dataset_config['z_overhead'] = 5\n",
    "val_dataset_config['overlap_levels'] = True\n",
    "val_dataset_config['y_overlap'] = 5\n",
    "\n",
    "val_dataset_config['x_overhead_axial'] = [2, 10]\n",
    "val_dataset_config['z_overhead_axial'] = 5\n",
    "val_dataset_config['overlap_levels_axial'] = True\n",
    "val_dataset_config['y_overlap_axial'] = 5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"print_evaluation\": True,\n",
    "    \"steps_per_plot\": 1,\n",
    "\n",
    "    \"checkpoints\": False,\n",
    "    \"save_path\": \"/workspaces/RSNA_LSDC/models_3d_final/model_weight\",\n",
    "    \"step_per_save\":100,\n",
    "    \"model_name\": \"densenet121_80_80_3_foraminas\",\n",
    "    \"train_dataset_config\": train_dataset_config,\n",
    "    \"val_dataset_config\": val_dataset_config,\n",
    "\n",
    "    \"epochs\": 30, \n",
    "    \"batch_size\": 5,\n",
    "\n",
    "    \"optimizer\": torch.optim.AdamW, #torch.optim.AdamW,#torch.optim.Adam,\n",
    "    \"optimizer_params\": {'lr': 3e-4, 'weight_decay': 1e-2},#, 'momentum': 0.98, 'weight_decay': 1e-3},#, 'momentum':0.98, 'weight_decay':1e-5},#, 'momentum':0.9},\n",
    "    \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, #torch.optim.lr_scheduler.ExponentialLR, #torch.optim.lr_scheduler.OneCycleLR\n",
    "    \"scheduler_params\": {'T_0': 15, 'T_mult': 1, 'eta_min':1e-8}, #{'T_0': 2, 'T_mult': 2, 'eta_min':3e-5}, #{'max_lr': 0.001, 'epochs': None, 'steps_per_epoch':None}, {'gamma':0.9}\n",
    "\n",
    "    \"early_stopping\": False,\n",
    "    \"early_stopping_treshold\": 0.1,\n",
    "    'vsa':True,\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'backbone_name': 'densenet121', \n",
    "    'series_dim': [train_dataset_config['series_len']]+train_dataset_config['im_size'],\n",
    "    'pretrained': True, \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')#.iloc[0:300]\n",
    "def kfoldCV(k, trainer_config, model_config):\n",
    "    model_summaries = []\n",
    "    data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "    data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "    data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "    unique_studies = np.random.permutation(np.array(tsd.study_id.unique()))\n",
    "    if k == 1:\n",
    "        with open('/workspaces/RSNA_LSDC/models_3d_final/test_unique_studies.npy', 'rb') as f:\n",
    "            test = np.load(f)\n",
    "        with open('/workspaces/RSNA_LSDC/models_3d_final/train_unique_studies.npy', 'rb') as f:\n",
    "            train = np.load(f) \n",
    "        folds = [test, train]\n",
    "    else:\n",
    "        folds = np.array_split(unique_studies, k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"Fold: {i}\")\n",
    "        train_ids = np.concatenate(folds[:i]+folds[i+1:], axis=0)\n",
    "        train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "        val_data=  {'sagittal': data_sagittal[data_sagittal.study_id.isin(folds[i])],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(folds[i])],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(folds[i])]}\n",
    "        \n",
    "        trainer = SagittalTrainer(ClassModelTimm2d, model_config, trainer_config, train_data, val_data)\n",
    "        trainer.train()\n",
    "        model_summaries.append(trainer.get_summary())\n",
    "\n",
    "    return model_summaries\n",
    "# convformer_s18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfoldCV(1, trainer_config, model_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
