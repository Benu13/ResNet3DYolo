{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import random\n",
    "from torch import nn\n",
    "\n",
    "from typing import List, Sequence, Tuple, Union, Dict\n",
    "from scipy import ndimage\n",
    "\n",
    "from MedicalNet.MedicalNet import Struct, MedNet\n",
    "import yolov9_head_func_3d as y9\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from torchvision.tv_tensors import BoundingBoxes as BB\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postprocessing found bboxes:\n",
    "Include guaranteed information about bboxes in image. \n",
    "-> 5 classes each with one bbox or 1 class with 5 bbox\n",
    "-> If there exist level n-1 and n+1 in image there must also exist level n\n",
    "-> The boxes of levels n-1, n, n+1 must be aligned next to another in heigh dimention\n",
    "-> The boxes must overlap in x dimention to some extend \n",
    "-> If there exist series of boxes for levels n, n+1, n+2 and image height is bigger than mean level heigh than the n+3 level must also exist - same situation in reverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bbox3d():\n",
    "    def __init__(self, x, y, z) -> None:\n",
    "        # 3d box in coordinates of sagittal series\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "    \n",
    "    def get_box_in_view_type(self, view_type, d3:bool=False):\n",
    "        if view_type in ['sagittal', 'sagittal_t2']:\n",
    "            return self.get_sagittal(d3)\n",
    "        elif view_type == 'coronal':\n",
    "            return self.get_coronal(d3)\n",
    "        elif view_type == 'axial':\n",
    "            return self.get_axial(d3)\n",
    "    \n",
    "    def get_sagittal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.x[0], self.y[0], self.x[1], self.y[1], self.z[0], self.z[1]]\n",
    "        return [self.x[0], self.y[0], self.x[1], self.y[1]]\n",
    "\n",
    "    def get_coronal(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.y[0], self.z[1], self.y[1], self.x[0], self.x[1]]\n",
    "        return [self.z[0], self.y[0], self.z[1], self.y[1]]\n",
    "\n",
    "    def get_axial(self, d3:bool=False):\n",
    "        if d3:\n",
    "            return [self.z[0], self.x[0], self.z[1], self.x[1], self.y[0], self.y[1]]\n",
    "        return [self.z[0], self.x[0], self.z[1], self.x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        # data_info: dict consisting of series types and dataframe with their info\n",
    "        # config: dict - dataset configuration\n",
    "        #TODO: Better overlap in instance dimention\n",
    "        #TODO: smart limit - selecting important slices based on series type and where they usually lay\n",
    "        \n",
    "        self.supress_warnings = config['supress_warinings']\n",
    "        self.used_series_types = config['load_series']\n",
    "        self.study_ids = np.unique(np.concatenate([series.study_id.unique() for series in data_info.values() if series is not None]))\n",
    "        self.data_info = data_info\n",
    "        if not np.all([tp in list(self.data_info.keys()) for tp in self.used_series_types]):\n",
    "            raise Exception(\"Series types to use do not match provided data information.\")\n",
    "        \n",
    "        self.series_out_types = config['series_out_types']\n",
    "        self.current_view = self.series_out_types[0]\n",
    "        self.return_series_type = config['return_series_type']\n",
    "\n",
    "        self.d3 = True #config['3d_box'] if '3d_box' in list(config.keys()) else False\n",
    "        self.preload = config['preload']\n",
    "        self.rng = np.random.default_rng()         \n",
    "        if self.preload and len(self.series_out_types)>1:\n",
    "            self.preload = False\n",
    "            if not self.supress_warnings:\n",
    "                print(\"Preloading with mixed series type outputs is not supported. Preloading was turned off.\")\n",
    "\n",
    "        self.im_size = config['im_size']\n",
    "        self.og_im_size = [20, 512, 512]\n",
    "\n",
    "        self.image_type = config['image_type']\n",
    "        self.rev_lr = config['rev'] if 'rev' in list(config.keys()) else False\n",
    "        if self.preload and not self.supress_warnings:\n",
    "            print(\"Warning! Preloading of images is turned on. The program will attempt to load whole dataset into memory!\")\n",
    "        \n",
    "        self.transforms = config['transforms']\n",
    "        self.normalize = config['normalize']\n",
    "        self.vsa = config['vsa']\n",
    "        \n",
    "        self.dataset_type = config['dataset_type']\n",
    "        self.dataset_path = config['dataset_path']\n",
    "\n",
    "        self._condition_list = ['Left Neural Foraminal Narrowing', \n",
    "                                'Left Subarticular Stenosis', \n",
    "                                'Right Neural Foraminal Narrowing', \n",
    "                                'Right Subarticular Stenosis', \n",
    "                                'Spinal Canal Stenosis']\n",
    "        \n",
    "        self.get_condition = [self._condition_list.index(cond) for cond in config['get_conditions']]\n",
    "        self.condition_to_get = config['get_conditions']\n",
    "        self._status_map = {'Normal/Mild': [1., 0., 0.],\n",
    "                            'Moderate': [0., 1., 0.],\n",
    "                            'Severe': [0., 0., 1.]}\n",
    "        \n",
    "        self.level_ind = {'L1/L2': 0, 'L2/L3': 1, 'L3/L4': 2, 'L4/L5': 3, 'L5/S1':4}\n",
    "\n",
    "        self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        self.limit = config['limit_series_len']\n",
    "        self.series_length = 15\n",
    "        \n",
    "        self.x_overhead = config['x_overhead'] if config['x_overhead'] else [30, 30]\n",
    "        self.z_overhead = config['z_overhead']\n",
    "        self.overlap_levels = config['overlap_levels']\n",
    "        self.y_overlap = config['y_overlap'] if self.overlap_levels else 0\n",
    "\n",
    "        self.cond_x_overhead = config['cond_x_overhead']\n",
    "        self.cond_y_overhead = config['cond_y_overhead']\n",
    "        self.cond_z_overhead = config['cond_z_overhead']\n",
    "\n",
    "        if self.limit and not config['series_len'] and not self.supress_warnings:\n",
    "            print(\"Series length is not specified. The limit is set to 15.\")\n",
    "        elif self.limit and config['series_len']:\n",
    "            self.series_length = config['series_len']\n",
    "            \n",
    "        self.series_type_ind = {}\n",
    "        for s in self.used_series_types:\n",
    "            self.series_type_ind[s] = []\n",
    "\n",
    "        self.data = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def get_level_boxes(self, series_info:pd.DataFrame, stype='sagittal'):\n",
    "        #default for each scan has 5 visible levels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        cond_boxes = []\n",
    "        cond_labels = []\n",
    "        z_overlap = self.z_overhead\n",
    "        if stype in ['sagittal', 'sagittal_t2']:\n",
    "            for _, row in series_info.iterrows():\n",
    "                sl = np.array(row.slice_locations)\n",
    "        \n",
    "                #z_overhead = -self.z_overhead if row.reversed else self.z_overhead\n",
    "                fr_a = sl[row.present_instances.index(min(row.instance_number))]\n",
    "                too_a = sl[row.present_instances.index(max(row.instance_number))]\n",
    "                fr = min(fr_a, too_a)\n",
    "                too = max(fr_a, too_a)\n",
    " \n",
    "                z_min = np.argmin(abs(sl - (fr-self.z_overhead )))\n",
    "                z_max = np.argmin(abs(sl - (too+self.z_overhead)))\n",
    "\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.x)-self.x_overhead[0]/row.pixel_spacing[0]), min(row.image_width, max(row.x)+self.x_overhead[1]/row.pixel_spacing[0])],\n",
    "                    y=[max(0, row.level_boundaries[0]-self.y_overlap/row.pixel_spacing[1]), min(row.image_height, row.level_boundaries[1]+self.y_overlap/row.pixel_spacing[1])],\n",
    "                    z=[min(z_max, z_min), max(z_max, z_min)]))\n",
    "                \n",
    "                c, cl = self.get_condition_boxes(row, stype=stype)\n",
    "                cond_boxes.append(c)\n",
    "                cond_labels.append(cl)\n",
    "\n",
    "        elif stype=='axial':\n",
    "            y_overlap = self.y_overlap + 0.5 \n",
    "            for _, row in series_info.iterrows():\n",
    "                labels.append(self.level_ind[row.level])\n",
    "                bboxes.append(Bbox3d(\n",
    "                    x=[max(0, min(row.y)-self.x_overhead[0]/row.pixel_spacing[1]), min(row.image_width, max(row.y)+self.x_overhead[1]/row.pixel_spacing[1])],\n",
    "                    y=[max(0, row.present_instances.index(min(row.level_slices))-y_overlap), min(len(row.present_instances), row.present_instances.index(max(row.level_slices))+y_overlap)],\n",
    "                    z=[max(0, min(row.x)-self.z_overhead/row.pixel_spacing[0]), min(row.image_height, max(row.x)+self.z_overhead/row.pixel_spacing[0])]\n",
    "                                ))\n",
    "\n",
    "                c, cl = self.get_condition_boxes(row, stype=stype)\n",
    "                cond_boxes.append(c)\n",
    "                cond_labels.append(cl)\n",
    "\n",
    "        return bboxes, np.array(labels), cond_boxes, cond_labels\n",
    "    \n",
    "    def get_condition_boxes(self, row:pd.DataFrame, stype='sagittal'):\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "\n",
    "        if stype in ['sagittal', 'sagittal_t2']:\n",
    "            for condition, x,y,z in zip(row.condition, row.x, row.y, row.instance_number):\n",
    "                if condition in self.condition_to_get:\n",
    "                    sl = np.array(row.slice_locations)\n",
    "                    #z_overhead = -self.z_overhead if row.reversed else self.z_overhead\n",
    "                    fr_a = sl[row.present_instances.index(z)]\n",
    "                    z_min = np.argmin(abs(sl - (fr_a-self.cond_z_overhead[condition][0])))\n",
    "                    z_max = np.argmin(abs(sl - (fr_a+self.cond_z_overhead[condition][1])))\n",
    "\n",
    "                    tmp = [(b,a) for a,b in zip(row.IPP, row.present_instances)]\n",
    "        \n",
    "                    reversed = True if tmp[0][1][0] > tmp[-1][1][0] else False\n",
    "                    if reversed:\n",
    "                        if \"Left\" in condition:\n",
    "                            zz = [3, 2]\n",
    "                        else:\n",
    "                            zz = [2, 3]\n",
    "                    else:\n",
    "                        if \"Left\" in condition:\n",
    "                            zz = [2, 3]\n",
    "                        else:\n",
    "                            zz = [3, 2]\n",
    "                          \n",
    "                    labels.append(self.condition_to_get.index(condition))\n",
    "                    bboxes.append(Bbox3d(\n",
    "                        x=[max(0, x-self.cond_x_overhead[condition][0]/row.pixel_spacing[0]), min(row.image_width, x+self.cond_x_overhead[condition][1]/row.pixel_spacing[0])],\n",
    "                        y=[max(0, y-self.cond_y_overhead[condition][0]/row.pixel_spacing[1]), min(row.image_height, y+self.cond_y_overhead[condition][1]/row.pixel_spacing[1])],\n",
    "                        z=[max(0, z-zz[0]), min(len(sl)-1, z+zz[1])]))\n",
    "                \n",
    "        elif stype=='axial':\n",
    "            for condition, x,y,z in zip(row.condition, row.x, row.y, row.instance_number):\n",
    "                if condition in self.condition_to_get:\n",
    "                    sl = np.array(row.slice_locations)\n",
    "                    fr_a = sl[row.present_instances.index(z)]\n",
    "                    z_min = np.argmin(abs(sl - (fr_a-self.cond_y_overhead[condition][0])))\n",
    "                    z_max = np.argmin(abs(sl - (fr_a+self.cond_y_overhead[condition][1])))\n",
    "\n",
    "                    labels.append(self.condition_to_get.index(condition))\n",
    "                    bboxes.append(Bbox3d(\n",
    "                        x=[max(0, y-self.cond_x_overhead[condition][0]/row.pixel_spacing[1]), min(row.image_width, y+self.cond_x_overhead[condition][1]/row.pixel_spacing[1])],\n",
    "                        y=[min(z_max, z_min), max(z_max, z_min)],#[max(0, row.present_instances.index(z)-self.cond_z_overhead[0]), min(len(row.level_instances), row.present_instances.index(z)+self.cond_z_overhead[1])],\n",
    "                        z=[max(0, x-self.cond_z_overhead[condition][0]/row.pixel_spacing[0]), min(row.image_height, x+self.cond_z_overhead[condition][1]/row.pixel_spacing[0])]\n",
    "                                    ))\n",
    "    \n",
    "        return bboxes, np.array(labels)\n",
    "\n",
    "    def set_view(self, new_view):\n",
    "        self.current_view = new_view\n",
    "        \n",
    "    def get_condition_labels(self, series_info:pd.DataFrame):\n",
    "        labels = []\n",
    "        cond_presence_masks = []\n",
    "        level_presence_mask = []\n",
    "\n",
    "        for level, _ in self.level_ind.items():\n",
    "            if not series_info[series_info['level']==level].empty:\n",
    "                labels.append(series_info[series_info['level']==level].iloc[0].status)\n",
    "                cond_presence_masks.append(series_info[series_info['level']==level].iloc[0].presence_mask)\n",
    "                level_presence_mask.append(True)\n",
    "            else:\n",
    "                level_presence_mask.append(False)\n",
    "\n",
    "        return np.array(labels), np.array(cond_presence_masks), np.array(level_presence_mask)\n",
    "\n",
    "    def info2dict(self, series_info, stype=None): #remember axials can be combination of different serieses (sagittals can't)\n",
    "        level0 = series_info.iloc[0]\n",
    "        data_dict = {}\n",
    "        boxes, box_labels, cond_boxes, cond_labels = self.get_level_boxes(series_info, stype=stype)\n",
    "        labels, label_level_mask, label_cond_mask = self.get_condition_labels(series_info)\n",
    "\n",
    "        data_dict['study_id'] = level0.study_id\n",
    "        data_dict['series_id'] = level0.series_id\n",
    "        data_dict['width'] = level0.image_width\n",
    "        data_dict['height'] = level0.image_height\n",
    "        data_dict['reversed'] = level0.reversed\n",
    "        data_dict['series_type'] = stype\n",
    "        data_dict['pixel_spacing'] = level0.pixel_spacing\n",
    "\n",
    "        data_dict['boxes'] = boxes \n",
    "        data_dict['files'] = [f\"{self.dataset_path}/{data_dict['study_id']}/{data_dict['series_id']}/{instance}.{self.image_type}\" for instance in level0.present_instances]\n",
    "        data_dict['box_labels'] = box_labels\n",
    "        data_dict['labels'] = labels\n",
    "        data_dict['label_presence_mask'] = label_level_mask\n",
    "        data_dict['cond_boxes'] = cond_boxes\n",
    "        data_dict['cond_labels'] = cond_labels\n",
    "        \n",
    "        return data_dict\n",
    "   \n",
    "    def prepare_data(self):\n",
    "        # prepare paths for every image to load\n",
    "        with tqdm(total=len(self.study_ids), desc=\"Preparing data: \") as pbar:\n",
    "            for study_id in self.study_ids:\n",
    "                study_dict = dict(\n",
    "                                sagittal=[], \n",
    "                                sagittal_t2=[], \n",
    "                                axial=[])\n",
    "                for stype in self.used_series_types:\n",
    "                    for series_id in self.data_info[stype].query(f'study_id == {study_id}').series_id.unique():\n",
    "                        ddict = self.info2dict(self.data_info[stype][self.data_info[stype].series_id==series_id], stype=stype)\n",
    "                        if self.preload:\n",
    "                            ddict = self.preload_series(ddict)\n",
    "                        study_dict[stype].append(ddict)\n",
    "\n",
    "                self.data.append(study_dict)\n",
    "                pbar.update(1)\n",
    "\n",
    "    def split_to_series(self):\n",
    "        temp = []\n",
    "        batches = []\n",
    "        i=0\n",
    "        for data in self.data:\n",
    "            batch = []\n",
    "            for series in data.values():\n",
    "                if series:\n",
    "                    temp+=series\n",
    "                    batch.append(i)\n",
    "                    i+=1\n",
    "            batches.append(batch)\n",
    "\n",
    "        self.data = temp\n",
    "        self.batches = batches\n",
    "    \n",
    "    def change_img_view(self, img, current_view, new_view):\n",
    "        if current_view in ['sagittal', 'sagittal_t2']:\n",
    "            if new_view in ['sagittal', 'sagittal_t2']:\n",
    "                return img\n",
    "            elif new_view=='coronal':\n",
    "                return img.transpose(2, 1, 0) # n, h, w -> w, h, n\n",
    "            elif new_view=='axial':\n",
    "                return img.transpose(1, 2, 0) #n, h, w -> h, n, w\n",
    "        elif current_view=='axial':\n",
    "            if new_view in ['sagittal', 'sagittal_t2']:\n",
    "                return img.transpose(2, 0, 1) # n, h, w -> w, n, h\n",
    "            elif new_view =='coronal':\n",
    "                return img.transpose(1, 0, 2) # n, h, w -> h, n, w\n",
    "            elif new_view=='axial':\n",
    "                return img\n",
    "            \n",
    "    def load_series(self, data) -> np.ndarray:\n",
    "        #print(data)\n",
    "        st = data['series_type']\n",
    "        boxes = np.array([box.get_box_in_view_type(st, d3 = True) for box in data['boxes']], dtype=int)\n",
    "        level_labels = data['box_labels']\n",
    "        cond_labels = data['labels']\n",
    "        #print(boxes)\n",
    "        oimg = np.zeros((len(data['files']), data['height'], data['width']), dtype = np.uint8)\n",
    "        for i, path in enumerate(data['files']):  \n",
    "            try:      \n",
    "                oimg[i,:,:] = np.array(Image.open(path), dtype = np.uint8)\n",
    "            except ValueError:\n",
    "                temp = np.array(Image.open(path), dtype = np.uint8) \n",
    "                oimg[i,:temp.shape[0],:temp.shape[1]] = temp\n",
    "                del temp\n",
    "\n",
    "        oimg = self.change_img_view(oimg, data['series_type'], st)\n",
    "        oimg = oimg\n",
    "        \n",
    "        imgs = np.zeros((5, self.series_length, self.im_size[1], self.im_size[0]))\n",
    "        condition_boxes = np.zeros((5, len(self.condition_to_get), 6))\n",
    "        condition_labels = np.zeros((5, len(self.condition_to_get)))\n",
    "\n",
    "        for i, (box, label, cond_label) in enumerate(zip(boxes, level_labels, cond_labels)):\n",
    "            try:\n",
    "                box[0] = int(max(0, box[0]+np.random.randint(-20, 20)))\n",
    "                box[1] = int(max(0, box[1]+np.random.randint(-20, 20)))\n",
    "                box[2] = int(min(oimg.shape[2], box[2]+np.random.randint(-20, 20)))\n",
    "                box[3] = int(min(oimg.shape[1], box[3]+np.random.randint(-20, 20)))\n",
    "                # box[4] = int(max(0, box[4]+np.random.randint(-1, 1)))\n",
    "                # box[5] = int(min(oimg.shape[0], box[5]+np.random.randint(-1, 1)))\n",
    "                lb = np.array([bb.get_box_in_view_type(st, d3 = True) for bb in data['cond_boxes'][i]], dtype=int)-np.array([box[0], box[1], box[0], box[1], box[4], box[4]])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            cl = data['cond_labels'][i]\n",
    "            for cll in cl:\n",
    "                condition_labels[i,cll]=np.argmax(cond_label[self._condition_list.index(self.condition_to_get[cll])])\n",
    "            ig, lb = self.__resize_data__(oimg[box[4]: box[5], box[1]:box[3], box[0]:box[2]], lb)\n",
    "            if 0 in ig.shape:\n",
    "                continue\n",
    "            imgs[label] = self.__itensity_normalize_one_volume__(ig)\n",
    "            condition_boxes[i,cl,:] = lb\n",
    "        #print(imgs, imgs.shape)\n",
    "\n",
    "        return imgs, condition_boxes, condition_labels\n",
    "    \n",
    "    \n",
    "    def __itensity_normalize_one_volume__(self, volume):\n",
    "        \"\"\"\n",
    "        normalize the itensity of an nd volume based on the mean and std of nonzeor region\n",
    "        inputs:\n",
    "            volume: the input nd volume\n",
    "        outputs:\n",
    "            out: the normalized nd volume\n",
    "        \"\"\"\n",
    "        #out = (volume - 0.5*255.)/(0.5 *255.)\n",
    "\n",
    "        pixels = volume[volume > 0]\n",
    "        if pixels.size==0:\n",
    "            return volume\n",
    "        \n",
    "        mean = pixels.mean()\n",
    "        std  = pixels.std()\n",
    "        out = (volume - mean)/std\n",
    "        #out_random = np.random.normal(0, 1, size = volume.shape)\n",
    "        #out[volume == 0] = out_random[volume == 0]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __resize_data__(self, data, bb=None):\n",
    "        \"\"\"\n",
    "        Resize the data to the input size\n",
    "        \"\"\" \n",
    "        \n",
    "        [depth, height, width] = data.shape\n",
    "        scale = [self.series_length*1.0/depth, self.im_size[1]*1.0/height, self.im_size[0]*1.0/width]\n",
    "        #scale_d = [self.series_length*1.0/depth, 1, 1]\n",
    "        if bb is not None:\n",
    "            bb[..., [0,2]] = bb[..., [0,2]]/width*self.im_size[0]\n",
    "            bb[..., [1,3]] = bb[..., [1,3]]/height*self.im_size[1]\n",
    "            bb[..., [4,5]] = bb[..., [4,5]]/depth*self.series_length\n",
    "\n",
    "        data = ndimage.zoom(data, scale, order=1)\n",
    "        #data = ndimage.zoom(data, scale_d, order=0)\n",
    "        #data, bb = self.limit_series(data, bb)\n",
    "        \n",
    "        if bb is not None:\n",
    "            return data, bb\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def limit_series(self, img, z_bb=None):\n",
    "        if self.series_length < img.shape[0]:\n",
    "                if self.d3 and z_bb:\n",
    "                    z_bb = z_bb* self.series_length/img.shape[0]\n",
    "                st = np.round(np.linspace(0, img.shape[0] - 1, self.series_length)).astype(int)\n",
    "                img = img[st,:,:]\n",
    "        elif self.series_length > img.shape[0]:\n",
    "                img = np.pad(img, ((0, self.series_length-img.shape[0]), (0, 0), (0, 0)))\n",
    "        if z_bb:\n",
    "            return img, z_bb\n",
    "        return img\n",
    "    \n",
    "    def coarse_dropout_3d(self, volume, max_holes_num, max_hole_size):\n",
    "        # set cut data from volume but leave parts around points with condition of intrest untoutched.\n",
    "        # draw number of holes\n",
    "        hol_num = self.rng.integers(1, max_holes_num)\n",
    "        # for each hole draw its placement on image and size\n",
    "        placement_and_size = self.rng.integers(low=0, \n",
    "                                      high = [[volume.shape[0], volume.shape[1], volume.shape[2], \n",
    "                                               max_hole_size[0], max_hole_size[1], max_hole_size[2]]]*hol_num)\n",
    "        for i in range(hol_num):\n",
    "            c = placement_and_size[i,:]\n",
    "            volume[c[0]:min(volume.shape[0], c[0]+c[3]), c[1]:min(volume.shape[1], c[1]+c[4]), c[2]: min(volume.shape[2], c[2]+c[5])] = 0.\n",
    "        return volume\n",
    "    \n",
    "    def prepare_series(self, img, boxes, labels):\n",
    "\n",
    "        #oimg = self.load_series(data, preloaded=self.preload)\n",
    "        level_labels = torch.arange(0,len(self.condition_to_get)) #torch.tensor(labels).reshape(-1)#torch.arange(0,len(self.condition_to_get))\n",
    "        if True: # unify foraminals with no regard to side\n",
    "             level_labels[level_labels==1] = 0 # right subarticular label as left\n",
    "             level_labels[level_labels==3] = 1 # right subarticular label as left\n",
    "             level_labels[level_labels==2] = 1 # right subarticular label as left\n",
    "        boxes = torch.tensor(boxes) \n",
    "\n",
    "        all_data = torch.cat([level_labels.unsqueeze(-1), boxes], dim = -1)\n",
    "        all_data = all_data[all_data[:,1:].sum(dim=-1) != 0]\n",
    "        boxes = all_data[:,1:]\n",
    "        level_labels = all_data[:,0]\n",
    "        \n",
    "        z_bb = boxes[..., [-2, -1]]\n",
    "        boxes = boxes[..., 0:4]\n",
    "            \n",
    "        boxes = BB(boxes, format='XYXY', canvas_size=(img.shape[1] , img.shape[2]), dtype=torch.float)\n",
    "        img = torch.tensor(img,dtype=torch.float)\n",
    "        target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": level_labels}\n",
    "        if self.transforms:\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": level_labels}\n",
    "            \n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        boxes = target['boxes']\n",
    "        level_labels = target['labels']\n",
    "        boxes = torch.cat([boxes, z_bb], dim=-1)[:, [0,1,4,2,3,5]]\n",
    "\n",
    "        if torch.rand(1) < 0.5 and self.vsa:\n",
    "            img = self.coarse_dropout_3d(img,12,[int(self.series_length/3),int(self.im_size[1]/3), int(self.im_size[0]/3)])\n",
    "\n",
    "        boxes[torch.logical_or(boxes[:,0] > img.shape[2]-img.shape[2]/5, boxes[:,3]-img.shape[2]/5 < 0)] *= 0\n",
    "        boxes[torch.logical_or(boxes[:,1] > img.shape[1]-img.shape[1]/5, boxes[:,4]-img.shape[1]/5 < 0)] *= 0\n",
    "       #boxes[torch.logical_or(boxes[:,3] > img.shape[0]-img.shape[0]/4, boxes[:,5]-img.shape[0]/4 < 0)] *= 0\n",
    "\n",
    "        boxes = boxes.clamp(\n",
    "            min=torch.tensor([0.,0.,0.,0.,0.,0.], dtype = torch.float), max=torch.tensor([img.shape[2]-1, img.shape[1]-1, img.shape[0]-1, img.shape[2]-1, img.shape[1]-1, img.shape[0]-1], dtype = torch.float))\n",
    "        if img.argmax() < 1:\n",
    "            boxes *=0\n",
    "        \n",
    "        return img, boxes, level_labels\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxDatasetUnited(Dataset):\n",
    "    def __init__(self, data_info:Dict[str, pd.DataFrame], config:Dict):\n",
    "        super(BoxDatasetUnited, self).__init__(data_info, config)\n",
    "\n",
    "        self.mix_strategy = config['mix_strategy']\n",
    "        if config['one_label']:\n",
    "            self.box_labels = torch.tensor([0,0,0,0,0], dtype=torch.float)\n",
    "        else:\n",
    "           self.box_labels = torch.tensor([list(self.level_ind.values())]) \n",
    "\n",
    "        #split data to individual serieses from dict of study-series_type pairs\n",
    "        self.split_to_series()\n",
    "    def __getitem__(self, index: int)->tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        adata = self.data[index]\n",
    "        #print(adata)\n",
    "        # limit series (if limit==True)\n",
    "        all_view_img = torch.zeros(5, self.series_length, self.im_size[1], self.im_size[0])\n",
    "        all_view_boxes = torch.zeros((5, len(self.condition_to_get), 6))\n",
    "        all_view_labels = torch.zeros((5, len(self.condition_to_get), 1))\n",
    "        try:\n",
    "            imgs, boxes, labels = self.load_series(adata)\n",
    "        except ZeroDivisionError: # one series was skipped in data preparation and now it raises exception ;_;\n",
    "            if self.return_series_type:\n",
    "                return all_view_img.to(dtype=torch.float), all_view_boxes.to(dtype=torch.float), all_view_labels.to(dtype=torch.float), adata['series_type']\n",
    "            return all_view_img.to(dtype=torch.float), all_view_boxes.to(dtype=torch.float), all_view_labels.to(dtype=torch.float)\n",
    "        \n",
    "        for i, (img, box, label) in enumerate(zip(imgs, boxes, labels)):\n",
    "            img, box, label= self.prepare_series(img, box, label)\n",
    "            if torch.count_nonzero(img) > 0:\n",
    "                all_view_img[i] = img\n",
    "                all_view_boxes[i,:box.shape[0]] = box\n",
    "                all_view_labels[i,:label.shape[0]] = label.unsqueeze(-1)\n",
    "                \n",
    "        if self.return_series_type:\n",
    "            return all_view_img, all_view_boxes, all_view_labels, adata['series_type']\n",
    "        \n",
    "        return all_view_img, all_view_boxes, all_view_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def get_all_random(self):\n",
    "        ai, ab, al, asl = [], [], [], []\n",
    "        for stype in self.used_series_types:\n",
    "            all_view_img, all_view_boxes, all_view_labels, s = self.get_random_by_stype(stype)\n",
    "            ai.append(all_view_img.unsqueeze(0))\n",
    "            ab.append(all_view_boxes.unsqueeze(0))\n",
    "            al.append(all_view_labels.unsqueeze(0))\n",
    "            asl.append(s)\n",
    "        return torch.cat(ai, dim=0), torch.cat(ab, dim=0),  torch.cat(al, dim=0),  asl\n",
    "\n",
    "    def get_random_by_stype(self, series_type):\n",
    "        if self.preload:\n",
    "            same_s_types = [ind for ind, data in enumerate(self.data) if data['o_series_type'] == series_type]\n",
    "        else:\n",
    "            same_s_types = [ind for ind, data in enumerate(self.data) if data['series_type'] == series_type]\n",
    "            \n",
    "        return self[random.choice(same_s_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 128\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5), v2.RandomHorizontalFlip(p = 0.5), v2.RandomAffine(degrees=5), \n",
    "                     v2.RandomRotation(degrees=(90,90)), v2.RandomRotation(degrees=(90,90))]),\n",
    "    v2.RandomChoice([v2.RandomPerspective(distortion_scale=0.2, p=1.0), v2.RandomAffine(degrees=0, translate=(0.3,0.3), shear=(-5,5,-5,5))]), # translation + shearing\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.ElasticTransform(alpha=40.0), v2.GaussianBlur(kernel_size=(5,5), sigma=(0.7, 0.7))]),\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "     v2.RandomRotation(degrees=(90,90)),\n",
    "])\n",
    "\n",
    "cond_x_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [6,9],\n",
    "    'Right Neural Foraminal Narrowing': [6,9],\n",
    "    'Left Subarticular Stenosis': [5,5],\n",
    "    'Right Subarticular Stenosis': [5,5],\n",
    "    'Spinal Canal Stenosis': [10, 10]\n",
    "}\n",
    "cond_y_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [12,12],\n",
    "    'Right Neural Foraminal Narrowing': [12,12],\n",
    "    'Left Subarticular Stenosis': [4,4],\n",
    "    'Right Subarticular Stenosis': [4,4],\n",
    "    'Spinal Canal Stenosis': [200, 200]\n",
    "}\n",
    "cond_z_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [8,8],\n",
    "    'Right Neural Foraminal Narrowing': [8,8],\n",
    "    'Left Subarticular Stenosis': [8,8],\n",
    "    'Right Subarticular Stenosis': [8,8],\n",
    "    'Spinal Canal Stenosis': [20, 20]\n",
    "}\n",
    "\n",
    "train_dataset_config ={\n",
    "    'preload': None, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [im_size, im_size],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset\",\n",
    "    'load_series': ['sagittal'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': train_transforms,\n",
    "    'vsa': True,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['sagittal', 'axial', 'sagittal_t2'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal'\n",
    "    'get_conditions':['Left Neural Foraminal Narrowing', 'Right Neural Foraminal Narrowing'], # condition to output from series\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    \n",
    "    'normalize': False,\n",
    "    'image_type': 'png',\n",
    "    'preload': False,\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 32, #  maximal number of slices in series\n",
    "\n",
    "    'x_overhead': [50,50], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead':59,\n",
    "    'overlap_levels': True, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 50, # overlap size of levels boundaries (in mm)\n",
    "    'cond_x_overhead': cond_x_overhead,\n",
    "    'cond_y_overhead': cond_y_overhead,\n",
    "    'cond_z_overhead': cond_z_overhead,\n",
    "\n",
    "    '3d_box': True\n",
    "}\n",
    "\n",
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv').iloc[0:100]\n",
    "\n",
    "data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "train_ids = tsd.study_id.unique()\n",
    "\n",
    "train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "            'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "            'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "\n",
    "bb = BoxDatasetUnited(train_data, train_dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = dict([(0,'g'),(1,'b'), (2,'r')])\n",
    "\n",
    "fig, ax = plt.subplots(5, 3, figsize = (30, 30))\n",
    "inputs, boxes, labels = bb[4]\n",
    "\n",
    "#imgs.append(inputs)\n",
    "# print ground truth\n",
    "for i, (img, box, label) in enumerate(zip(inputs.squeeze(), boxes.squeeze(), labels.squeeze())):\n",
    "    ax[i,0].imshow(img[int(box[0,5]),:,:].detach().cpu().numpy())\n",
    "    ax[i,1].imshow(img.permute(2,1,0)[int(box[0,2]+box[0,5])//2,:,:].detach().cpu().numpy())\n",
    "    ax[i,2].imshow(img.permute(2,0,1)[16,:,:].detach().cpu().numpy())\n",
    "\n",
    "    for j in range(box.shape[0]):\n",
    "        l = label[j].numpy()\n",
    "        #color = color_dict[int(l)]\n",
    "\n",
    "        b_sag = box[j,[0,1,3,4]].numpy()\n",
    "        b_cor = box[j,[2,1,5,4]].numpy()\n",
    "        b_ax  = box[j,[1,2,4,5]].numpy()\n",
    "        ax[i,0].add_patch(patches.Rectangle((b_sag[0], b_sag[1]), b_sag[2]-b_sag[0], b_sag[3]-b_sag[1], linewidth=1, edgecolor = color_dict[int(l)], facecolor='none'))\n",
    "        ax[i,1].add_patch(patches.Rectangle((b_cor[0], b_cor[1]), b_cor[2]-b_cor[0], b_cor[3]-b_cor[1], linewidth=1, edgecolor = color_dict[int(l)], facecolor='none'))\n",
    "        ax[i,2].add_patch(patches.Rectangle((b_ax[0], b_ax[1]), b_ax[2]-b_ax[0], b_ax[3]-b_ax[1], linewidth=1, edgecolor = color_dict[int(l)], facecolor='none'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SagittalTrainer():\n",
    "    def __init__(self, model, model_params, config, train_data, eval_data) -> None:\n",
    "\n",
    "        self.print_evaluation = config[\"print_evaluation\"] if \"print_evaluation\" in config else False\n",
    "        self.steps_per_plot = config[\"steps_per_plot\"]\n",
    "\n",
    "        if config['train_dataset_config']['mix_strategy'] and len(config['train_dataset_config']['series_out_types'])>1:\n",
    "            print(\"Warning! With mix strategy set to 'combined' and multiple series out types the batch size will be (len(series_out_types)) times bigger.\")\n",
    "\n",
    "        self.checkpoints = config['checkpoints']\n",
    "        self.save_path = config['save_path']\n",
    "        self.step_per_save = config['step_per_save']\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Device set to {self.device}\")\n",
    "\n",
    "        self.model_name = config['model_name']\n",
    "        self.model = model(**model_params).to(self.device)\n",
    "        #self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = config[\"optimizer\"](self.model.parameters(),**config[\"optimizer_params\"])\n",
    "\n",
    "        self.series_len = config['val_dataset_config']['series_len']\n",
    "        self.dataloaders = {'train': torch.utils.data.DataLoader(BoxDatasetUnited(train_data, config['train_dataset_config']), batch_size=config[\"batch_size\"], shuffle=True, num_workers=12, prefetch_factor=1),\n",
    "                           'val': torch.utils.data.DataLoader(BoxDatasetUnited(eval_data, config['val_dataset_config']), batch_size=config[\"batch_size\"], shuffle=False, num_workers=12, prefetch_factor=1)}\n",
    "        \n",
    "        self.max_epochs = config[\"epochs\"]\n",
    "        self.early_stopping = config['early_stopping']\n",
    "        self.early_stopping_tresh = config['early_stopping_treshold']\n",
    "\n",
    "        # scheduler\n",
    "        if config[\"scheduler\"]:\n",
    "            if 'epochs' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['epochs'] = self.max_epochs\n",
    "            if 'steps_per_epoch' in list(config['scheduler_params'].keys()):\n",
    "                config['scheduler_params']['steps_per_epoch'] = len(self.dataloaders['train'])\n",
    "            self.scheduler = config[\"scheduler\"](self.optimizer,**config[\"scheduler_params\"])\n",
    "            self.one_cycle_sched = self.scheduler.__class__.__name__ == 'OneCycleLR'\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        ## Evaluation metrics\n",
    "        self.series_in_types = config['train_dataset_config']['load_series']\n",
    "\n",
    "        self.out_stypes = config['train_dataset_config']['series_out_types']\n",
    "        self.batch_size = config[\"batch_size\"] * len(self.out_stypes)\n",
    "        self.condition_to_get = config['train_dataset_config']['get_conditions']\n",
    "        self.mAP = MeanAveragePrecision(box_format = 'xyxy', iou_type='bbox', extended_summary=True).to(self.device)\n",
    "        self.all_maps = []\n",
    "        self.mAP_split = []\n",
    "        self.best_map = 0.\n",
    "        self.metrics_to_print = ['map', 'map_50', 'map_75']\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.save_path, f\"{self.model_name}_best.pt\"))\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.max_epochs}\")\n",
    "            self.train_one_epoch()\n",
    "            self.eval_one_epoch()\n",
    "            #print examples\n",
    "            if (epoch+1)%self.steps_per_plot==0:\n",
    "                self.plot_examples()\n",
    "            #checkpoint\n",
    "            if epoch%5==0:\n",
    "                pass\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "\n",
    "        self.model.train()  # Set model to training mode\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "        with tqdm(self.dataloaders['train'], unit = \"batch\",\n",
    "                    total = len(self.dataloaders['train'])) as tepoch:\n",
    "            for inputs, boxes, labels in self.dataloaders['train']:\n",
    "                inputs = inputs.to(self.device).reshape(-1, self.series_len, inputs.shape[-2], inputs.shape[-1])\n",
    "                boxes = boxes.to(self.device).reshape(-1, len(self.condition_to_get), 6)\n",
    "                labels = labels.to(self.device).reshape(-1, len(self.condition_to_get), 1)\n",
    "                valid = [i for i in range(inputs.shape[0]) if (inputs[i].argmax() > 0 and boxes[i].argmax()>0)]\n",
    "                if len(valid) <1:\n",
    "                    tepoch.update(1)\n",
    "                    continue\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    loss, loss_info = self.model.get_loss(inputs[valid], boxes[valid], labels[valid])\n",
    "                    for loss_t, loss_v in loss_info.items():\n",
    "                        metrics[loss_t].append(loss_v.clone().detach().cpu().numpy())\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler and self.one_cycle_sched:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                #update tqdm data\n",
    "                tepoch.set_description(self.metrics_description(metrics, 'train'))\n",
    "                tepoch.update(1)\n",
    "\n",
    "        if self.scheduler and not self.one_cycle_sched:\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "    def eval_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        targets = []\n",
    "        preds = []\n",
    "        stypes = []\n",
    "\n",
    "        alabels = []\n",
    "        apreds = []\n",
    "        aweight = []\n",
    "\n",
    "        with tqdm(self.dataloaders['val'], unit = \"batch\",\n",
    "                            total = len(self.dataloaders['val'])) as tepoch:\n",
    "            for inputs, boxes, labels, stype in self.dataloaders['val']: \n",
    "                with torch.set_grad_enabled(False):   \n",
    "                    inputs = inputs.to(self.device).reshape(-1, self.series_len, inputs.shape[-2], inputs.shape[-1])\n",
    "                    boxes = boxes.to(self.device).reshape(-1, len(self.condition_to_get), 6)\n",
    "                    labels = labels.to(self.device).reshape(-1, len(self.condition_to_get), 1)\n",
    "                    \n",
    "                    valid = [i for i in range(inputs.shape[0]) if (inputs[i].argmax() > 0 and boxes[i].argmax()>0)]\n",
    "                    if len(valid) <1:\n",
    "                        tepoch.update(1)\n",
    "                        continue\n",
    "                    if len(inputs.shape) <4:\n",
    "                        tepoch.update(1)\n",
    "                        continue\n",
    "                    inputs = inputs[valid]\n",
    "                    boxes = boxes[valid]\n",
    "                    labels = labels[valid]\n",
    "                    \n",
    "                    out = self.model.predict(inputs.to(self.device).reshape(-1, self.series_len, inputs.shape[-2], inputs.shape[-1])) # reshape input to accomodate multiple view outputs\n",
    "                    # sort apreds by depth\n",
    "                    for o in out:\n",
    "                        sort_ind = o['boxes'][:,0].argsort()\n",
    "                        apreds.append(o['logits'][sort_ind])\n",
    "                    \n",
    "                    # sort labels from min depth to max depth\n",
    "                    sorted_labels = []\n",
    "                    for label, box in zip(labels.reshape(-1,2), boxes.reshape(-1,2,6)):\n",
    "                        sort_ind = box[:,0].argsort(-1)\n",
    "                        sorted_labels.append(label[sort_ind])\n",
    "                    \n",
    "                    slabels = torch.cat(sorted_labels)\n",
    "                    slabels=  labels.reshape(-1)\n",
    "                    weights = 2**slabels\n",
    "                    alabels.append(slabels)\n",
    "                    aweight.append(weights)\n",
    "\n",
    "                    for pred in out:\n",
    "                        pred['boxes']= pred['boxes'][..., [0,1,3,4]]\n",
    "\n",
    "                    preds += out\n",
    "                    targets += [dict(boxes=box.to(self.device)[(box != 0).any(dim=-1).nonzero()].squeeze(1), \n",
    "                                     labels=label.to(self.device)[(box != 0).any(dim=-1).nonzero()].squeeze((1,2)).to(torch.int)) \n",
    "                                     for box, label in zip(boxes[..., [0,1,3,4]].reshape(-1,len(self.condition_to_get),4), labels.reshape(-1,len(self.condition_to_get),1))]\n",
    "                    \n",
    "                    stypes += [st for st in stype]\n",
    "                    tepoch.update(1)\n",
    "\n",
    "        alabels = torch.cat(alabels, dim=0).cpu().numpy()\n",
    "        apreds = torch.cat(apreds, dim=0).cpu().numpy()\n",
    "        aweight=torch.cat(aweight, dim=0).cpu().numpy()\n",
    "\n",
    "        # #prind confusion matrix for every condition\n",
    "        # conditions = ['Left Foramina', 'Right Foramina']\n",
    "        \n",
    "        # fig, ax = plt.subplots(nrows=1, ncols=len(conditions), figsize=(15,5))\n",
    "        # if len(conditions) > 1:\n",
    "        #     ax = ax.ravel()\n",
    "        # else:\n",
    "        #     ax = [ax]\n",
    "        # for i in range(len(conditions)):\n",
    "        #     cl = alabels[i::len(conditions)]\n",
    "        #     cpred = apreds[i::len(conditions),:].argmax(-1)\n",
    "        #     cm = confusion_matrix(cl, cpred)\n",
    "        #     ax[i].set_title(conditions[i])\n",
    "        #     ConfusionMatrixDisplay(\n",
    "        #         confusion_matrix=cm).plot(ax=ax[i], colorbar=False)\n",
    "        # plt.show()\n",
    "\n",
    "        # ll = log_loss(alabels, apreds, normalize=True, sample_weight=aweight)\n",
    "        # if ll < self.best_ll:\n",
    "        #     self.save_model()\n",
    "        #     self.best_ll = ll\n",
    "\n",
    "        self.mAP.update(preds=preds, target=targets)\n",
    "        all_maps = {k: v for k, v in self.mAP.compute().items()}\n",
    "        self.all_maps.append(all_maps)\n",
    "        self.mAP.reset()\n",
    "        if all_maps['map'] > self.best_map:\n",
    "            self.save_model()\n",
    "            self.best_map = all_maps['map']\n",
    "        #print validation metrics\n",
    "        validation_log =[]\n",
    "        validation_log.append(\"All mAP validation metrics: \" + self.get_map_str(all_maps))\n",
    "        # divide further by in series types\n",
    "       \n",
    "        #print(\"Score:\", ll)\n",
    "        if self.print_evaluation:\n",
    "            for str in validation_log:\n",
    "                print(str)\n",
    "\n",
    "    \n",
    "    def get_map_str(self, map: Dict):\n",
    "        str = \"\"\n",
    "        for k, v in map.items():\n",
    "            if k in self.metrics_to_print:\n",
    "                str+= f\"{k}: {v}\"\n",
    "                str+= \" || \"\n",
    "        return str\n",
    "\n",
    "    def plot_examples(self, plot_num:int=1):\n",
    "        color_dict = dict([(0,'r'),(1,'g'), (2,'b'), (3,'m'), (4, 'y'), (5, 'r'),  (6, 'r'),  (7, 'r'),  (8, 'r'), (9, 'r')])\n",
    "        self.model.eval()\n",
    "        for i in range(plot_num):#sin_type in self.series_in_types[0]:\n",
    "            fig, ax = plt.subplots(3, 2*5, figsize = (16, 8))\n",
    "            for a in ax.ravel():\n",
    "                a.set_axis_off()\n",
    "                a.set_yticklabels([])\n",
    "                a.set_xticklabels([])\n",
    "\n",
    "            #if len(self.out_stypes)<2:\n",
    "             #   ax = np.expand_dims(ax, axis=1)\n",
    "                \n",
    "            #fig.suptitle(f\"Example plot for input series with type {sin_type}.\", fontsize=12)\n",
    "            inputs, boxes, labels, _ = self.dataloaders['val'].dataset.get_all_random()\n",
    "            #level = int(random.choice(list(range(0,5))))\n",
    "            #inputs = inputs[:, level,...]\n",
    "            #boxes = boxes[:, level,...]\n",
    "            #labels = labels[:, level,...]\n",
    "            with torch.set_grad_enabled(False):\n",
    "                preds = self.model.predict(inputs.to(self.device).reshape(-1, self.series_len, inputs.shape[-2], inputs.shape[-1])) # predict \n",
    "            # print ground truth\n",
    "            for i, (img, box, label, pred) in enumerate(zip(inputs.squeeze(), boxes.squeeze(), labels.squeeze(), preds)):\n",
    "                ax[0,0+2*i].imshow(img[int(box[0,2]+box[0,5])//2,:,:].detach().cpu().numpy())\n",
    "                ax[1,0+2*i].imshow(img[int(box[0,2]+box[0,5])//2,:,:].detach().cpu().numpy())\n",
    "                ax[2,0+2*i].imshow(img[int(box[1,2]+box[1,5])//2,:,:].detach().cpu().numpy())\n",
    "\n",
    "                ax[0,1+2*i].imshow(img.permute(1,2,0)[int(self.series_len/2),:,:].detach().cpu().numpy())\n",
    "                ax[1,1+2*i].imshow(img.permute(1,2,0)[int(self.series_len/2),:,:].detach().cpu().numpy())\n",
    "                ax[2,1+2*i].imshow(img.permute(1,2,0)[int(self.series_len/2),:,:].detach().cpu().numpy())\n",
    "\n",
    "                for j in range(box.shape[0]):\n",
    "                    b_sag = box[j,[0,1,3,4]].numpy()\n",
    "                    b_cor = box[j,[2,0,5,3]].numpy()\n",
    "                    l = label[j].numpy()\n",
    "                    color = color_dict[int(l)]\n",
    "                    ax[0,0+2*i].add_patch(patches.Rectangle((b_sag[0], b_sag[1]), b_sag[2]-b_sag[0], b_sag[3]-b_sag[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "                    ax[0,1+2*i].add_patch(patches.Rectangle((b_cor[0], b_cor[1]), b_cor[2]-b_cor[0], b_cor[3]-b_cor[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "\n",
    "                for j in range(len(pred['boxes'])):\n",
    "                    b_sag = pred['boxes'][j, [0,1,3,4]].cpu().numpy()\n",
    "                    b_cor = pred['boxes'][j, [2,0,5,3]].cpu().numpy() #[j, [2,1,5,4]].cpu().numpy()\n",
    "                    l = pred['labels'][j].cpu().numpy()\n",
    "                    color = color_dict[int(l)]\n",
    "                    ax[1,0+2*i].add_patch(patches.Rectangle((b_sag[0], b_sag[1]), b_sag[2]-b_sag[0], b_sag[3]-b_sag[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "                    ax[1,1+2*i].add_patch(patches.Rectangle((b_cor[0], b_cor[1]), b_cor[2]-b_cor[0], b_cor[3]-b_cor[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "\n",
    "                    ax[2,0+2*i].add_patch(patches.Rectangle((b_sag[0], b_sag[1]), b_sag[2]-b_sag[0], b_sag[3]-b_sag[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "                    ax[2,1+2*i].add_patch(patches.Rectangle((b_cor[0], b_cor[1]), b_cor[2]-b_cor[0], b_cor[3]-b_cor[1], linewidth=1, edgecolor=color, facecolor='none'))\n",
    "\n",
    "            plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "            plt.show()\n",
    "                \n",
    "\n",
    "    def metrics_description(self, metrics:dict, phase:str)->str:\n",
    "        outputs = phase + \": ||\"\n",
    "        for k in metrics.keys():\n",
    "            outputs += (\" {}: {:4f} ||\".format(k, np.mean(metrics[k])))\n",
    "        return outputs\n",
    "    \n",
    "    def get_summary(self, desc:str = None):\n",
    "        # desc (str): additional description to include\n",
    "        \n",
    "        summary = {\n",
    "            'model_name': self.model_name,\n",
    "            'backbone': self.model.backbone_name,\n",
    "            'conv_type': self.model.head_conv_mode,\n",
    "            'use_reg_for_cls': self.model.use_reg_for_cls,\n",
    "            'mix_strategy': self.config['train_dataset_config']['mix_strategy']\n",
    "        }\n",
    "        #find epoch with best mAP\n",
    "        all_map = [epm['map'] for epm in self.all_maps]\n",
    "        best_epoch = np.argmax(all_map)\n",
    "        summary['best_epoch'] = best_epoch\n",
    "\n",
    "        # append general metrics\n",
    "        for key, val in self.all_maps[best_epoch].items():\n",
    "            if key in self.metrics_to_print:\n",
    "                summary[f\"general_{key}\"] = val\n",
    "        \n",
    "        if self.mAP_split:\n",
    "            for map_split_in_name, map_split_in in self.mAP_split[best_epoch].items():\n",
    "                for map_split_out_name, map_split_out in map_split_in.items():\n",
    "                    for key, val in map_split_out.items():\n",
    "                        if key in self.metrics_to_print:\n",
    "                            summary[f\"{map_split_in_name}_{map_split_out_name}_{key}\"] = val\n",
    "        if desc:\n",
    "            summary['description'] = desc\n",
    "            \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineinter(line1, line2):\n",
    "    inter = (torch.min(line1[5], line2[:,5]) - torch.max(line1[2], line2[:,2])).clamp(0)\n",
    "    uni = (torch.max(line1[5], line2[:,5]) - torch.min(line1[2], line2[:,2])).clamp(0) - inter\n",
    "    return inter/(uni + 1e-7)\n",
    "\n",
    "class Nms3dSagittalForamina(nn.Module):\n",
    "    def __init__(self, iou_treshold=0.3, score_treshold = 0., del_same_depth=False, unique_cls = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.score_tr = score_treshold\n",
    "        self.iou_tr = iou_treshold\n",
    "        self.del_same_depth = del_same_depth\n",
    "        self.unique_cls = unique_cls\n",
    "\n",
    "    def forward(self, results):\n",
    "        # boxes in format [score, label, x, y, z, x, y, z]\n",
    "        aresult = results.clone().detach()\n",
    "        out = []\n",
    "        for i in range(self.unique_cls): # max two objects per unique class\n",
    "            result = aresult[aresult[:,1]==i]\n",
    "            result = result[result[:,0].argsort(dim=0, descending=True)] # sort by score \n",
    "            result = result[result[:,0] >= self.score_tr] # score tr\n",
    "            \n",
    "            if result.nelement() == 0:\n",
    "                return result\n",
    "            \n",
    "            filtered = []\n",
    "            while result.nelement() != 0:\n",
    "                filtered.append(result[[0]])\n",
    "                ious = y9.bbox_iou(result[0, 2:2+6], result[:, 2:2+6], iou_mode=True)\n",
    "                if self.del_same_depth:\n",
    "                    li = lineinter(result[0, 2:2+6], result[:, 2:2+6])\n",
    "                    result = result[torch.logical_and(ious.squeeze()<=self.iou_tr, li < 0.1)]\n",
    "                else:\n",
    "                    result = result[ious.squeeze()<=self.iou_tr]\n",
    "\n",
    "            result = torch.cat(filtered, dim=0)\n",
    "            out.append(result[0:2]) #limit to max 2 foraminas per image\n",
    "\n",
    "        return torch.cat(out, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxModel(nn.Module):\n",
    "    def __init__(self, backbone_name:str, series_dim:list[int], num_classes:int=1,\n",
    "                 use_features:Union[str, list[int]]=[0], reg_max:int=16, pretrained:bool=False,\n",
    "                 head_conv_mode:str = '2d', use_reg_for_cls:bool = False):\n",
    "        # backbone_name - timm model to use as backbone\n",
    "        # series_dim - dimentionality of the series [num_channels, im_width, im_height]\n",
    "        # use_features - features to use from backbone output:\n",
    "        #        example model outputs features with dim [64, 64, 128, 256, 512] \n",
    "        #               'last' or [0] will take only last layer\n",
    "        #               'all' will take all layers\n",
    "        #               [0, 1, 2] will take last three layers\n",
    "\n",
    "        super().__init__()\n",
    "        # model outputs bounding box in yolo format [x_mid, y_mid, width, height] (normalized)\n",
    "        # chose feature extractor from timm models\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.num_classes = num_classes\n",
    "        self.series_dim = series_dim\n",
    "        self.backbone_name = backbone_name\n",
    "        self.joint_train = False\n",
    "\n",
    "        opts = {\n",
    "        'model': 'resnet',\n",
    "        'input_W': 128,\n",
    "        'input_H': 128,\n",
    "        'input_D': 32,\n",
    "        'device': self.device,\n",
    "        'phase': 'train',\n",
    "        }\n",
    "        model_pretrained_params = {\n",
    "            'resnet_10': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "            'resnet_10_23dataset': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "            'resnet_18': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "            'resnet_18_23dataset': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "            'resnet_34': {'model_depth': 34, 'resnet_shortcut': 'A'},\n",
    "            'resnet_34_23dataset': {'model_depth': 34, 'resnet_shortcut': 'A'}\n",
    "        }\n",
    "        for model_name, model_dict in model_pretrained_params.items():\n",
    "            model_pretrained_params[model_name] = Struct({**model_dict, **opts})\n",
    "\n",
    "        def construct_network(feature_extractor, model_pretrained_params):\n",
    "            model = MedNet(feature_extractor, model_pretrained_params)\n",
    "            return model\n",
    "\n",
    "        self.feature_extractor = construct_network('resnet_18_23dataset', model_pretrained_params).to(self.device)\n",
    "        self.feature_extractor.init_FE(self.device, '/workspaces/RSNA_LSDC/MedicalNet/Pretrained/resnet_18_23dataset.pth')\n",
    "\n",
    "        self.all_channels = [64, 128, 256, 512]\n",
    "        self.reduction = [4, 8, 8, 8]\n",
    "\n",
    "        # self.series_dim = series_dim\n",
    "        # self.backbone_name = backbone_name\n",
    "        # self.feature_extractor = timm_3d.create_model(\n",
    "        #                                 backbone_name,\n",
    "        #                                 pretrained=True,\n",
    "        #                                 features_only=True,\n",
    "        #                                 in_chans=1\n",
    "        #                             )\n",
    "                                            \n",
    "        # self.all_channels = self.feature_extractor.feature_info.channels()\n",
    "        # self.reduction = self.feature_extractor.feature_info.reduction() #[4,8,16,32] \n",
    "\n",
    "        print(self.all_channels, self.reduction)\n",
    "\n",
    "        # features dimention\n",
    "        if use_features=='all':\n",
    "            self.in_channels = self.all_channels\n",
    "            self.featmap_stride = self.reduction\n",
    "            self.fl = list(range(len(self.all_channels)))\n",
    "        elif use_features=='last':\n",
    "            self.in_channels = [self.all_channels[-1]]\n",
    "            self.featmap_stride = [self.reduction[-1]]\n",
    "            self.fl = [len(self.all_channels)-1]\n",
    "        elif type(use_features)==list:\n",
    "            self.in_channels = [self.all_channels[len(self.all_channels)-(1+i)] for i in sorted(use_features, reverse=True)]\n",
    "            self.featmap_stride = [self.reduction[len(self.reduction)-(1+i)] for i in sorted(use_features, reverse=True)]\n",
    "            self.fl= [len(self.all_channels)-(1+i) for i in sorted(use_features, reverse=True)]\n",
    "\n",
    "        self.head_conv_mode = head_conv_mode\n",
    "        self.use_reg_for_cls = use_reg_for_cls\n",
    "\n",
    "        self.postprocess = Nms3dSagittalForamina(0.1, 0.1, True, 1)\n",
    "        self.head = y9.Detect3d(nc=num_classes, ch=self.in_channels, strides=self.featmap_stride, reg_max=reg_max, return_logits=True)\n",
    "        h = {\n",
    "            \"device\": self.device,\n",
    "            \"cls_pw\":None,\n",
    "            \"label_smoothing\": 0.0,\n",
    "            \"fl_gamma\": 0.0,\n",
    "            'bbox_weight': 5.5,\n",
    "            'class_weight': .5,\n",
    "            'dfl_weight':3.,\n",
    "            #'weight': [1., 2., 4.]\n",
    "        }\n",
    "        self.CL = y9.ComputeLoss(self.head, h)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[List]:\n",
    "        \"\"\"Forward features from the upstream network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input series\n",
    "        Returns:\n",
    "            Tuple[List]: A tuple of multi-level classification scores, bbox\n",
    "            predictions\n",
    "        \"\"\"\n",
    "    \n",
    "        x = self.feature_extractor(x.unsqueeze(1))\n",
    "        x = [x[i] for i in self.fl]\n",
    "\n",
    "        # for j in x:\n",
    "        #     print(j.shape)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "    def get_loss(self, series, gt_boxes, gt_labels):\n",
    "        head_out = self.forward(series)  \n",
    "        loss, loss_split, pred_a_boxes = self.CL(head_out,\n",
    "                                        gt_boxes, gt_labels)\n",
    "\n",
    "        metrics = dict(loss_cls=loss_split[1], loss_bbox=loss_split[0], loss_dfl=loss_split[2])\n",
    "        \n",
    "        return loss, metrics\n",
    "    \n",
    "    def get_predictions(self, series, gt_boxes, gt_labels):\n",
    "        result_list = []\n",
    "        head_out = self.forward(series)  \n",
    "        loss, loss_split, pred_a_boxes = self.CL(head_out,\n",
    "                                        gt_boxes, gt_labels)\n",
    "        #print(pred_a_boxes.shape)\n",
    "        for result in pred_a_boxes:\n",
    "            if result.nelement() == 0:\n",
    "                result_list.append({'boxes': result, 'scores': result, 'labels': result.int(), 'logits': result})\n",
    "            else:\n",
    "                result_list.append({'boxes': result[:, 2:], 'scores': result[:, 0], 'labels': result[:, 1].int()})\n",
    "        return result_list\n",
    "\n",
    "    def predict(self, series):\n",
    "        scores, labels, dbox, logits = self.forward(series)\n",
    "        result_list = []\n",
    "        for score, label, boxes, logit in zip(scores, labels, dbox, logits):\n",
    "            result = self.postprocess(torch.cat((score, label, boxes, logit), dim=0).permute(1,0))\n",
    "            result_list.append({'boxes': result[:, 2:], 'scores': result[:, 0], 'labels': result[:, 1].int(), 'logits': result[:, 8:].softmax(-1)})\n",
    "        return result_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 96\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomChoice([v2.RandomVerticalFlip(p = 0.5), v2.RandomHorizontalFlip(p = 0.5), v2.RandomAffine(degrees=5), \n",
    "                     v2.RandomRotation(degrees=(90,90)), v2.RandomRotation(degrees=(-90,-90))]),\n",
    "    v2.RandomChoice([v2.RandomPerspective(distortion_scale=0.2, p=1.0), v2.RandomAffine(degrees=0, translate=(0.3,0.3), shear=(-5,5,-5,5))]), # translation + shearing\n",
    "    v2.RandomAffine(degrees=0, scale=(0.8,1.2)), #scaling\n",
    "    v2.RandomChoice([v2.ElasticTransform(alpha=40.0), v2.GaussianBlur(kernel_size=(5,5), sigma=(0.7, 0.7))]),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.RandomVerticalFlip(p = 0.),\n",
    "    #v2.Resize((im_size,im_size)), #resize\n",
    "])\n",
    "\n",
    "cond_x_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [6,9],\n",
    "    'Right Neural Foraminal Narrowing': [6,9],\n",
    "    'Left Subarticular Stenosis': [5,5],\n",
    "    'Right Subarticular Stenosis': [5,5],\n",
    "    'Spinal Canal Stenosis': [10, 10]\n",
    "}\n",
    "cond_y_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [11,15],\n",
    "    'Right Neural Foraminal Narrowing': [11,15],\n",
    "    'Left Subarticular Stenosis': [4,4],\n",
    "    'Right Subarticular Stenosis': [4,4],\n",
    "    'Spinal Canal Stenosis': [200, 200]\n",
    "}\n",
    "cond_z_overhead = {\n",
    "    'Left Neural Foraminal Narrowing': [12,12],\n",
    "    'Right Neural Foraminal Narrowing': [12,12],\n",
    "    'Left Subarticular Stenosis': [8,8],\n",
    "    'Right Subarticular Stenosis': [8,8],\n",
    "    'Spinal Canal Stenosis': [20, 20]\n",
    "}\n",
    "train_dataset_config ={\n",
    "    'preload': True, # preload data into memory (WARNING IT MAY TAKE A LOT OF SPACE - DEPENDS ON THE DATASET USED)\n",
    "    'im_size': [im_size, im_size],\n",
    "    'dataset_path': \"/workspaces/RSNA_LSDC/inputs/dataset\",\n",
    "    'load_series': ['sagittal'], # series types to load into dataset ['sagittal', 'axial', 'sagittal_t2']\n",
    "    'united': True,\n",
    "\n",
    "    'transforms': train_transforms,\n",
    "    'vsa': False,\n",
    "    'one_label': False, # use one label for every level (do not differentiate between levels)\n",
    "    'series_out_types': ['sagittal'], # mix output series types to get views necessary to create 3d box ['sagittal', 'coronal'\n",
    "    'get_conditions':['Left Neural Foraminal Narrowing', 'Right Neural Foraminal Narrowing'], # condition to output from series\n",
    "    'mix_strategy': 'combined', # strategy for mixing output series types ['random', 'custom', 'combined'] random - randomly select output type, \n",
    "                              #'manual' - will return data based on currently choosen view, 'combined' will return all views in one call\n",
    "    'return_series_type': False, # If True getitem will also return series orignial type\n",
    "    \n",
    "    'normalize': True,\n",
    "    'image_type': 'png',\n",
    "    'preload': False,\n",
    "\n",
    "    'dataset_type': 'boxes', #'boxes', 'conditions'\n",
    "    'supress_warinings': False, \n",
    "\n",
    "    'limit_series_len': True, # if True the series length will be limited to number N specified by 'series_len' parameter\n",
    "    'series_len': 48, #  maximal number of slices in series\n",
    "\n",
    "    'x_overhead': [25,25], # overhead for levels in x-dim (in mm)\n",
    "    'z_overhead':25,\n",
    "    'overlap_levels': True, # if true the level upper and lower boundary will overlap with value specified in 'y_overlap'\n",
    "    'y_overlap': 5, # overlap size of levels boundaries (in mm)\n",
    "    'cond_x_overhead': cond_x_overhead,\n",
    "    'cond_y_overhead': cond_y_overhead,\n",
    "    'cond_z_overhead': cond_z_overhead,\n",
    "\n",
    "    '3d_box': True\n",
    "}\n",
    "\n",
    "val_dataset_config = copy.deepcopy(train_dataset_config)\n",
    "val_dataset_config['transforms'] = val_transforms\n",
    "val_dataset_config['mix_strategy'] = 'combined'\n",
    "val_dataset_config['vsa'] = False\n",
    "val_dataset_config['rev'] = True\n",
    "val_dataset_config['return_series_type'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"print_evaluation\": True,\n",
    "    \"steps_per_plot\": 1,\n",
    "\n",
    "    \"checkpoints\": False,\n",
    "    \"save_path\": \"/workspaces/RSNA_LSDC/model_weight\",\n",
    "    \"step_per_save\":100,\n",
    "\n",
    "    \"model_name\": \"foramina_detect_sagittal_mednet18_96_96_48\",\n",
    "    \"train_dataset_config\": train_dataset_config,\n",
    "    \"val_dataset_config\": val_dataset_config,\n",
    "\n",
    "    \"optimizer\": torch.optim.AdamW,#torch.optim.Adam,\n",
    "    \"optimizer_params\": {'lr': 2e-4},#, 'momentum':0.98, 'weight_decay':1e-5},#, 'momentum':0.9},\n",
    "    \"scheduler\": torch.optim.lr_scheduler.ExponentialLR,#torch.optim.lr_scheduler.ExponentialLR, #torch.optim.lr_scheduler.OneCycleLR,\n",
    "    \"scheduler_params\":{'gamma':0.99},#{'max_lr': 0.001, 'epochs': None, 'steps_per_epoch':None}, {'gamma':0.9}\n",
    "\n",
    "    \"epochs\": 15, \n",
    "    \"batch_size\":2,\n",
    "    \"early_stopping\": False,\n",
    "    \"early_stopping_treshold\": 0.1\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'backbone_name': 'densenet121', \n",
    "    'series_dim': [train_dataset_config['series_len']]+train_dataset_config['im_size'],\n",
    "    'use_features': [0,1],\n",
    "\n",
    "    'reg_max': 16, \n",
    "    'pretrained': False, \n",
    "    'num_classes': 1, #1 if train_dataset_config['one_label'] else 5,\n",
    "    'head_conv_mode': '3d',\n",
    "    'use_reg_for_cls': False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = pd.read_csv(f'/workspaces/RSNA_LSDC/inputs/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')#.iloc[0:300]\n",
    "def kfoldCV(k, trainer_config, model_config):\n",
    "    model_summaries = []\n",
    "    data_sagittal = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t1.pkl\")\n",
    "    data_sagittal_t2 = pd.read_pickle(\"/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_unified_sagital_t2.pkl\")\n",
    "    data_axial = pd.read_pickle('/workspaces/RSNA_LSDC/inputs/box_data/coordinates/coordinates_axial_unified.pkl')\n",
    "    unique_studies = np.random.permutation(np.array(tsd.study_id.unique()))\n",
    "    if k == 1:\n",
    "        with open('/workspaces/RSNA_LSDC/inputs/train_unique_studies.npy', 'rb') as f:\n",
    "            train = np.load(f)\n",
    "        with open('/workspaces/RSNA_LSDC/inputs/test_unique_studies.npy', 'rb') as f:\n",
    "            test = np.load(f) \n",
    "        folds = [test, train]\n",
    "    else:\n",
    "        folds = np.array_split(unique_studies, k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"Fold: {i}\")\n",
    "        train_ids = np.concatenate(folds[:i]+folds[i+1:], axis=0)\n",
    "        train_data={'sagittal': data_sagittal[data_sagittal.study_id.isin(train_ids)],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(train_ids)],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(train_ids)]}\n",
    "        val_data=  {'sagittal': data_sagittal[data_sagittal.study_id.isin(folds[i])],\n",
    "                    'sagittal_t2': data_sagittal_t2[data_sagittal_t2.study_id.isin(folds[i])],\n",
    "                    'axial': data_axial[data_axial.study_id.isin(folds[i])]}\n",
    "        \n",
    "        trainer = SagittalTrainer(BoxModel, model_config, trainer_config, train_data, val_data)\n",
    "        trainer.train()\n",
    "        model_summaries.append(trainer.get_summary())\n",
    "\n",
    "    return model_summaries\n",
    "# convformer_s18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "kfoldCV(1, trainer_config, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if __name__ == '__main__':\n",
    "    kfoldCV(1, trainer_config, model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MedicalNet.MedicalNet import Struct, MedNet\n",
    "import torch\n",
    "\n",
    "opts = {\n",
    "'model': 'resnet',\n",
    "'input_W': 224,\n",
    "'input_H': 224,\n",
    "'input_D': 32,\n",
    "'device': 'cuda',\n",
    "'n_seg_classes': 4,\n",
    "'phase': 'train',\n",
    "}\n",
    "\n",
    "model_pretrained_params = {\n",
    "    'resnet_10': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "    'resnet_10_23dataset': {'model_depth': 10, 'resnet_shortcut': 'B'},\n",
    "    'resnet_18': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "    'resnet_18_23dataset': {'model_depth': 18, 'resnet_shortcut': 'A'},\n",
    "    'resnet_34': {'model_depth': 34, 'resnet_shortcut': 'A'},\n",
    "    'resnet_34_23dataset': {'model_depth': 34, 'resnet_shortcut': 'A'}\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, model_dict in model_pretrained_params.items():\n",
    "    model_pretrained_params[model_name] = Struct({**model_dict, **opts})\n",
    "\n",
    "def construct_network(feature_extractor, model_pretrained_params):\n",
    "    model = MedNet(feature_extractor, model_pretrained_params)\n",
    "    return model\n",
    "\n",
    "model = construct_network('resnet_34_23dataset', model_pretrained_params).to('cuda')\n",
    "model.init_FE('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(torch.rand((1,1,32,224,224)).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIMM 3D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
